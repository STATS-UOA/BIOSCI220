# Multivariate Data Methods

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE
)
```

## Learning Objectives

+ Explain the aims and motivation behind Principal Component Analysis (PCA) and its relevance in biology
+ Explain the aims and motivation behind cluster analysis and its relevance in biology
+ Write `R` code to carry out PCA
+ Write `R` code to carry out k-means cluster analysis
+ Interpret `R` output from PCA and k-means cluster analysis
+ Interpret and communicate, to both a statistical and non-statistical audience, multivariate data techniques, specifically,
    + PCA
    + k-means clustering

## Dimension reduction

Reduction of dimensions is needed when there are far too many features in a dataset. Too many features makes it hard to distinguish between the important ones that are relevant to the output and the redundant or not-so important ones. Reducing the dimensions of data is called **dimensionality reduction.**

So the aim is to **find the best low-dimensional representation of the variation** in a multivariate (lots and lots of variables) data set, but how do we do this? 

One way is termed Principal Component Analysis (PCA). PCA is a **feature extraction** method that **reduces the dimensionality of the data** (number of variables) by creating new uncorrelated variables while minimizing loss of information on the original variables.

**Think of a baguette.** The baguette pictured here represents two data dimensions: 1) the length of the bread and 2) the height of the bread (we'll ignore depth of bread for now). Think of the baguette as your data; when we carry out PCA we're rotating our original axes (*x- and y-coordinates*) to capture as much of the variation in our data as possible. This results in **new** uncorrelated variables that each explain a \% of variation in our data; the procedure is designed so that the first new variable (PC1) explains the most, the second (PC2) the second most and so on.

![](https://raw.githubusercontent.com/cmjt/statbiscuits/master/figs_n_gifs/pca.gif)


Now rather than a baguette think of data; the baguette above represent the *shape* of the scatter between the two variables plotted below. The rotating grey axes represent the PCA procedure, essentially searching for the *best* rotation of the original axes to represent the variation in the data as best it can. Mathematically the Euclidean distance (e.g., the distance between points $p$ and $q$ in Euclidean space, $\sqrt{(p-q)^2}$) between the points and the rotating axes is being minimized (i.e., the shortest possible across all points), see the blue lines. Once this distance is minimized across all points we "settle" on our new axes (the black tiled axes).

![](https://raw.githubusercontent.com/cmjt/statbiscuits/master/figs_n_gifs/perp.gif)

Luckily we can do this all in `R`!

## PCA  in `R`

**Using the `palmerpenguins` data**

```{r}
## we should be used to loading these packages by now :-)
library(tidyverse)
library(palmerpenguins)
## getting rid of NAs
penguins_nafree <- penguins %>% drop_na()
```

When carrying out PCA we're only interested in numeric variables, so let's just plot those. We can use the piping operator `%>%` to do this with out creating a new data frame

```{r}
## introducing a new package GGally, please install
## using install.packages("GGally")
library(GGally)
penguins_nafree %>%
  select(species, where(is.numeric)) %>% 
  ggpairs(columns = c("flipper_length_mm", "body_mass_g", 
                     "bill_length_mm", "bill_depth_mm")) 

```

We see that a lot of these variables (e.g., `flipper_length_mm`, `body_mass_g`, and `bill_length_mm`) are relatively strongly (positively) related to one another. Could they actually be telling us the same information? Combined we could think of these three variables all telling us a little about *bigness* of penguin. Is there a way we could reduce these three variables, into say 1, to represent the *bigness* of a penguin. We may not need *all* the information (variation) captured by these variables, but could get away with fewer *new uncorrelated* variables that represent basically the same information (e.g., penguin *bigness*), thereby,  **reducing the dimensionality of the data**.


### Using `prcomp()` to carry out PCA

As often the data variables are on very different scales (e.g., grams and millimeters) we should first make sure that each variable is on the same **scale**. We call this scaling and can easily do this in `R` using the function `scale()`. Typically, we scale a variable, $x$, by subtracting its mean, $\mu_x$, (i.e., centering it) and dividing by its standard deviation, $\sigma_x$ a measure of its spread: $\frac{x - \mu_x}{\sigma_x}$.This ensures that each variable is on the same scale (i.e., changes are relative).

Now, there are three basic types of information we obtain from Principal Component Analysis:

  + **PC scores:**  the coordinates of our samples on the new PC axis: the **new uncorrelated variables** (stored in `pca$x`)

  + **Eigenvalues:** represent the variance explained by each PC; we can use these to calculate the proportion of variance in the original data that each axis explains

  + **Variable loadings** (eigenvectors): these reflect the weight that each variable has on a particular PC and can be thought of as the correlation between the PC and the original variable
  
```{r}
pca <- penguins_nafree %>% 
  select(where(is.numeric), -year) %>% ## year makes no sense here so we remove it and keep the other numeric variables
  scale() %>% ## scale the variables
  prcomp()
## print out a summary
summary(pca)
```
  
This output tells us that we obtain 4 principal components, which are called `PC1` `PC2`, `PC3`, and `PC4` (this is as expected because we used the 4 original numeric variables!). Each of these `PC`s explains a percentage of the total variation (`Proportion of Variance`) in the dataset: 
 
 + `PC1` explains $\sim$ 68\% of the total variance, which means that just over half of the information in the dataset 
 (5 variables) can be encapsulated by just that one Principal Component. 
 + `PC2` explains $\sim$ 19\% of the variance.
 + `PC3` explains $\sim$ 9\% of the variance.
 + `PC4` explains $\sim$ 2\% of the variance.
 
 
From the `Cumulative Proportion` row we see that by knowing the position of a sample in relation to just `PC1` and `PC2` we can get a pretty accurate view on where it stands in relation to other samples, as just `PC1` and `PC2`  explain 88\% of the variance.


The **loadings** (*relationship*) between the initial variables and the principal components are stored in `pca$rotation`:

```{r}
pca$rotation
```

Here we can see that `bill_length_mm` has a strong positive relationship with `PC1`, whereas `bill_depth_mm` has a strong negative relationship. Both `fliper_length_mm` and `body_mass_g` also have a strong positive relationship with `PC1`. Think back to the `ggpairs` plot above, `bill_length_mm`, `fliper_length_mm` and `body_mass_g` clearly had a strong positive relationship and we chose to refer to the combination as penguin *bigness*. We also saw that `bill_depth_mm` had a somewhat negative relationship with those three variables (somehow represents the opposite of penguin *bigness*).

There are two ways we can illustrate these **loadings** (*relationship*) between the initial variables and the principal components:

By showing the absolute value

```{r, echo = FALSE}
pca$rotation %>%
  as.data.frame() %>%
  mutate(variables = rownames(.)) %>%
  gather(PC,loading,PC1:PC4) %>%
  ggplot(aes(abs(loading), variables, fill = loading > 0)) +
  geom_col() +
  facet_wrap(~PC, scales = "free_y") +
  labs(x = "Absolute value of loading",y = NULL, fill = "Positive?") 

```

or by plotting them alongside new variables (PCs) (stored in `pca$x`):
  
```{r}
library(factoextra) ## install this package first
fviz_pca_biplot(pca, geom = "point") +
      geom_point (alpha = 0.2)
```

The plot above is called a **biplot**. By default the first two **new uncorrelated** variables are plotted (`PC1` and `PC2`). `PC1` is on the x-axis and again we can see it accounts for 68.6% of the variation. Roughly speaking the length of the arrows labeled by the original variables represents the magnitude of the *loadings* on the new variables and where they point indicates the direction of that relationship (i.e., negative or positive). 

Looking at the `PC1` axis we can see again that `bill_length_mm`, `fliper_length_mm` and `body_mass_g` all have a large positive relationship with `PC1`, so let's call `PC1` penguin *bigness*. As noted above, `bill_depth_mm` is negatively correlated with `PC1` (points in the negative direction) .

Now let's look at the `PC2` (y) axis; `PC2` accounts for just 19.5% of the variation. We note that neither `fliper_length_mm` nor `body_mass_g` have much of a relationship with `PC2` both arrows are pretty much on the zero line (see the barplot above as well). However, both `bill_length_mm` and `bill_depth_mm` have a relatively strong negative relationship with `PC2`, so we could think of this new variable as representing penguin *billness* (bill shape).

Now what about `PC2` vs `PC3`...

```{r}
fviz_pca_biplot(pca, axes = c(2,3),geom = "point") +
      geom_point (alpha = 0.2)
```

What do you think this is showing?

### Choosing what to keep

**But how many PCs (new variables) do we keep?** The whole point of this exercise is to **reduce** the number of variables we need to explain the variation in our data. So how many of these new variables (PCs) do we keep?

To assess this we can use the proportion of variance explained; recall that we want as much variance as possible explained whilst reducing the number of variables we keep. We can plot the proportion of variance explained, this is typically referred to as a **screeplot**.

```{r, message=FALSE}
## print out a summary
summary(pca)
## plot this as a screeplot
fviz_screeplot(pca)
```

Looking at these outputs and given the information above we note that `PC1` explains almost 70% of the variation in the four variables; `PC`s 2, 3 and 4 then explain the other 30% combined. Now given that our aim is to **reduce** the number of variables whilst **retaining** as much of the information as possible I'd be happy with just `PC1` (i.e., penguin *bigness*) representing mu data. The rule of thumb is to look for the *elbow* in the screeplot; that is, the point (`PC`) beyond which you are only explaining incrementally more variation. In the plot above we can see that this *elbow* happens at `PC2` backing up my claim to simply retain just `PC1` (penguin *bigness*.

### Principal components from the original variables

Recall that the principal components are a linear combination of the (statndardised) variables. So for `PC1`

```{r}
loadings1 <- pca$rotation[,1]
loadings1
```

Therefore, the first Principle Component will be $0.454\times Z1 -0.399 \times Z2 + 0.5768 \times Z3 + 0.5497 \times Z3$ where $Z1$, $Z2$, $Z3$. and $Z4$ are the scaled numerical variables form the penguins dataset (i.e., `r names(loadings1)`). To compute this we use `R`:

```{r}
scaled_vars <- penguins_nafree %>% 
  select(where(is.numeric), -year) %>% 
  scale() %>%
  as_tibble()
## By "Hand"
by_hand <- loadings1[1]*scaled_vars$"bill_length_mm" + 
  loadings1[2]*scaled_vars$"bill_depth_mm" + 
  loadings1[3]*scaled_vars$"flipper_length_mm" + 
  loadings1[4]*scaled_vars$"body_mass_g"
## From PCA
pc1 <- pca$x[,1]
plot(by_hand,pc1)
```



## Clustering

So, it's all about variation again! And the idea of minimizing it.


K-means clustering involves defining clusters so that the overall variation within a cluster (known as total within-cluster variation) is minimized. How do we define this variation? Typically, using Euclidean distances; the total within-cluster variation, is in this case, is defined as the sum of squared distances Euclidean distances between observations and the corresponding cluster centroid. 


In summary, this is the procedure

+ The number of clusters (k) are specified
+ k objects from the dataset are selected at random and *set* as the initial cluster centers or means
+ Each observation is assigned to their closest centroid (*based on the Euclidean distance between the object and the centroid*)
+ For each of the k clusters the cluster centroid is then updated based on calculating the new mean values of all the data points in the cluster
+ Repeat the two previous steps until 1) the cluster assignments stop changing or 2) the maximum number of iterations is reached



**Identify optimal number of clusters**


Identifying the appropriate k is important because too many or too few clusters impedes viewing overall trends. Too many clusters can lead to over-fitting (which limits generalizations) while insufficient clusters limits insights into commonality of groups.

There are assorted methodologies to identify the appropriate $k$. Tests range from blunt visual inspections to robust algorithms. The optimal number of clusters is ultimately a **subjective decision**.

### K-means in `R` using `factoextra`

```{r, message =FALSE}
## library for k-means clustering, will need to install first
library(factoextra)
df <- penguins_nafree %>%
  select(where(is.numeric), -year)
```
We use the `kmeans()` function from the `factoextra` package.

The first argument of `kmeans()` should be the dataset you wish to cluster. Below we use data frame `df`, the penguin data discussed above. But how many clusters do we choose? Let's try 1 to 5... (i.e., using the `centers` argument). Setting `nstart = 25` means that R will try 25 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation. 


```{r}
## set the seed so we all start off in the same place
set.seed(4321)
## one cluster
k1 <- kmeans(df, centers = 1, nstart = 25)
## two clusters
k2 <- kmeans(df, centers = 2, nstart = 25)
## three clusters
k3 <- kmeans(df, centers = 3, nstart = 25)
## four clusters
k4 <- kmeans(df, centers = 4, nstart = 25)
## five clusters
k5 <- kmeans(df, centers = 5, nstart = 25)
```


The `kmeans()` function returns a list of components:

+ `cluster`, integers indicating the cluster to which each observation is allocated
+ `centers`, a matrix of cluster centers/means
+ `totss`, the total sum of squares
+ `withinss`, within-cluster sum of squares, one component per cluster
+ `tot.withinss`, total within-cluster sum of squares
+ `betweenss`, between-cluster sum of squares
+ `size`, number of observations in each cluster

#### Choosing the number of clusters

We have an idea there may be 3 clusters, perhaps, but how do we know this is the best fit? Remember it's a **subjective choice** and we'll be looking at a few pointers

**Visual inspection** method

```{r}
p1 <- fviz_cluster(k1, data = df)
p2 <- fviz_cluster(k2, data = df)
p3 <- fviz_cluster(k3, data = df)
p4 <- fviz_cluster(k4, data = df)
p5 <- fviz_cluster(k5, data = df)

## for arranging plots
library(patchwork) 
(p1| p2| p3)/ (p4 | p5)
```

Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables.

```{r, echo = TRUE}
df %>%
  mutate(cluster = k3$cluster,
         species = penguins_nafree$species) %>%
  ggplot(aes(flipper_length_mm, bill_depth_mm, color = factor(cluster), label = species)) +
  geom_text()
```

**Elbow** method

Optimal clusters are at the point in which the knee "bends" or in mathematical terms when the marginal total within sum of squares (`tot.withinss`) for an additional cluster begins to decrease at a linear rate

This is easier to see via a plot:

```{r}
fviz_nbclust(df, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")
```

There is a pretty obvious inflection (elbow) at 2 clusters, but maybe at 3 too. We can rule out an optimal number of clusters above 3 as there is then only a  minimal marginal reduction in total within sum of squares. However, the model is ambiguous on whether 2 or 3 clusters is optimal...



**Silhouette** method

```{r}
# Silhouette method
fviz_nbclust(df, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

```


**Gap** method

```{r}
# Gap statistic
# recommended value: nboot = 500 for your analysis (it will take a while)
set.seed(123) ## remove this
fviz_nbclust(df, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")
```


**Basically it's up to you to collate all the suggestions and make and informed decision**

```{r}
## Trying all the cluster indecies AHHHHH
library(NbClust)
cluster_30_indexes <- NbClust(data = df, distance = "euclidean", min.nc = 2, max.nc = 9, method = "complete", index ="all")
fviz_nbclust(cluster_30_indexes) +
      theme_minimal() +
      labs(title = "Frequency of Optimal Clusters using 30 indexes in NbClust Package")
```

Not obvious, basically still undecided between 2 and 3, but according to the absolute majority rule the "best" number is 3

## TL;DR k-means clustering

**[Artwork by \@allison_horst](https://github.com/allisonhorst/stats-illustrations)**

![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_1.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_2.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_3.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_4.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_5.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_6.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_7.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_8.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_9.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_10.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_11.jpg)
![](https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_12.jpg)



## Other resources: optional but recommended

+ [Eigenfaces](https://cmjt.github.io/statbiscuits/eigenfaces.html)

+ [ClusterDucks](https://cmjt.github.io/statbiscuits/clusterducks.html)

+ [Little book for Multivariate Analysis](https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/index.html)

+ ['explor' is an R package to allow interactive exploration of multivariate analysis results](https://juba.github.io/explor/)

+ [The Mathematics Behind Principal Component Analysis (6 min read)](https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643)

+ [K-means cluster analysis](https://uc-r.github.io/kmeans_clustering)


### Multidimensional Scaling in `R` (*not examinable*)


Multidimensional scaling (MDS) is actually the more general technique of dimension reduction. PCA is a special case of MDS! 

To carry out MDS in `R`

```{r,eval = TRUE}
library(ggfortify)
## Plotting Multidimensional Scaling (for interest)
## stats::cmdscale performs Classical MDS
data("eurodist") ## road distances (in km) between 21 cities in Europe.
autoplot(eurodist)
## Plotting Classical (Metric) Multidimensional Scaling
autoplot(cmdscale(eurodist, eig = TRUE))
autoplot(cmdscale(eurodist, eig = TRUE), label = TRUE, shape = FALSE,
         label.size = 3)
## Plotting Non-metric Multidimensional Scaling
## MASS::isoMDS and MASS::sammon perform Non-metric MDS
library(MASS)
autoplot(sammon(eurodist))
autoplot(sammon(eurodist), shape = FALSE, label = TRUE,label.size = 3)
## Have a go at interpreting these plots based on the geography of the cities :-)
```