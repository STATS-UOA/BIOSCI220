<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Multivariate Data Methods | Module I BIOSCI220, University of Auckland</title>
  <meta name="description" content="6 Multivariate Data Methods | Module I BIOSCI220, University of Auckland" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Multivariate Data Methods | Module I BIOSCI220, University of Auckland" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Multivariate Data Methods | Module I BIOSCI220, University of Auckland" />
  
  
  

<meta name="author" content="Charlotte Jones-Todd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-the-design-and-analysis-of-experiments.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#course-overview-biosci220"><i class="fa fa-check"></i><b>0.1</b> Course Overview: BIOSCI220</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#module-1-key-topics"><i class="fa fa-check"></i><b>0.2</b> Module 1: Key Topics</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#timetable-assessment-modules-i-ii-and-iii"><i class="fa fa-check"></i><b>0.3</b> Timetable &amp; Assessment (Modules I, II, and III)</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> <code>R</code> and <code>RStudio</code></a><ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>1.2</b> Introduction to <code>R</code> and <code>RStudio</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#why"><i class="fa fa-check"></i><b>1.2.1</b> Why?</a></li>
<li class="chapter" data-level="1.2.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.2.2</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#getting-started"><i class="fa fa-check"></i><b>1.2.3</b> Getting started</a></li>
<li class="chapter" data-level="1.2.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-scripts-a-.r-file"><i class="fa fa-check"></i><b>1.2.4</b> R <em>Script</em>s (a <code>.r</code> file)</a></li>
<li class="chapter" data-level="1.2.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#writing-comments"><i class="fa fa-check"></i><b>1.2.5</b> Writing <em>Comments</em></a></li>
<li class="chapter" data-level="1.2.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-types"><i class="fa fa-check"></i><b>1.2.6</b> Data types</a></li>
<li class="chapter" data-level="1.2.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#creating-objects"><i class="fa fa-check"></i><b>1.2.7</b> Creating <em>Objects</em></a></li>
<li class="chapter" data-level="1.2.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-functions"><i class="fa fa-check"></i><b>1.2.8</b> <code>R</code> functions</a></li>
<li class="chapter" data-level="1.2.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-packages"><i class="fa fa-check"></i><b>1.2.9</b> <code>R</code> packages</a></li>
<li class="chapter" data-level="1.2.10" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-errors"><i class="fa fa-check"></i><b>1.2.10</b> <code>R</code> Errors</a></li>
<li class="chapter" data-level="1.2.11" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#working-directories"><i class="fa fa-check"></i><b>1.2.11</b> Working directories</a></li>
<li class="chapter" data-level="1.2.12" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#getting-help"><i class="fa fa-check"></i><b>1.2.12</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#exploratory-data-analysis-eda"><i class="fa fa-check"></i><b>1.3</b> Exploratory Data Analysis (EDA)</a><ul>
<li class="chapter" data-level="1.3.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#reading-in-data-from-a-.csv-file"><i class="fa fa-check"></i><b>1.3.1</b> Reading in data from a <code>.csv</code> file</a></li>
<li class="chapter" data-level="1.3.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-functions-to-explore-the-data"><i class="fa fa-check"></i><b>1.3.2</b> Using functions to explore the data</a></li>
<li class="chapter" data-level="1.3.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-plotting-for-your-own-purposes"><i class="fa fa-check"></i><b>1.3.3</b> Basic plotting (for your own purposes)</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#other-resources-optional-but-recommended"><i class="fa fa-check"></i><b>1.4</b> Other resources: optional but recommended</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html"><i class="fa fa-check"></i><b>2</b> Data exploration and visualization</a><ul>
<li class="chapter" data-level="2.1" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#learning-objectives-1"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#data-sovereignty"><i class="fa fa-check"></i><b>2.2</b> Data sovereignty</a><ul>
<li class="chapter" data-level="2.2.1" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#mƒÅori-data-sovereignty-principles"><i class="fa fa-check"></i><b>2.2.1</b> MƒÅori Data Sovereignty principles</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#data-wrangling-and-manipulation"><i class="fa fa-check"></i><b>2.3</b> Data wrangling and manipulation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#introuducing-the-palmer-penguins"><i class="fa fa-check"></i><b>2.3.1</b> Introuducing the <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081">Palmer penguins</a></a></li>
<li class="chapter" data-level="2.3.2" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#common-dataframe-manipulations-in-the-tidyverse-using-dplyr-and-tidyr"><i class="fa fa-check"></i><b>2.3.2</b> Common dataframe manipulations in the <code>tidyverse</code>, using <code>dplyr</code> and <code>tidyr</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#data-viz"><i class="fa fa-check"></i><b>2.4</b> Data Viz</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#exploratory-and-explanatory-plots"><i class="fa fa-check"></i><b>2.4.1</b> Exploratory and explanatory plots</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#ten-simple-rules-for-better-figures"><i class="fa fa-check"></i><b>2.4.2</b> <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003833">Ten Simple Rules for Better Figures</a></a></li>
<li class="chapter" data-level="2.4.3" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#introducing-ggplot2"><i class="fa fa-check"></i><b>2.4.3</b> Introducing <code>ggplot2</code></a></li>
<li class="chapter" data-level="2.4.4" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#what-do-we-think-about-when-we-look-at-plots-a-taster"><i class="fa fa-check"></i><b>2.4.4</b> What do we think about when we look at plots (<em>a taster</em>)</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#other-resources-optional-but-recommended-1"><i class="fa fa-check"></i><b>2.5</b> Other resources: optional but recommended</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Hypothesis testing and introduction to linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#learning-objectives-2"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#introdcution-to-hypothesis-testing"><i class="fa fa-check"></i><b>3.2</b> Introdcution to hypothesis testing</a><ul>
<li class="chapter" data-level="3.2.1" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#one-sample-t-test"><i class="fa fa-check"></i><b>3.2.1</b> One-Sample t-test</a></li>
<li class="chapter" data-level="3.2.2" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#differences-between-two-means"><i class="fa fa-check"></i><b>3.2.2</b> Differences between two means</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#correctly-interpreting-p-values"><i class="fa fa-check"></i><b>3.3</b> üò± Correctly interpreting p-values üò±</a><ul>
<li class="chapter" data-level="3.3.1" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#the-asa-statement-on-p-values-context-process-and-purpose"><i class="fa fa-check"></i><b>3.3.1</b> <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108">The ASA Statement on p-Values: Context, Process, and Purpose</a></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#power-significance-and-multiple-comparisons"><i class="fa fa-check"></i><b>3.4</b> Power, Significance, and multiple comparisons</a></li>
<li class="chapter" data-level="3.5" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#randomization-test"><i class="fa fa-check"></i><b>3.5</b> Randomization test</a></li>
<li class="chapter" data-level="3.6" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#one-way-anova-using-lm"><i class="fa fa-check"></i><b>3.6</b> One way ANOVA using <code>lm()</code></a></li>
<li class="chapter" data-level="3.7" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#model-diagnostics"><i class="fa fa-check"></i><b>3.7</b> Model diagnostics</a></li>
<li class="chapter" data-level="3.8" data-path="hypothesis-testing-and-introduction-to-linear-regression.html"><a href="hypothesis-testing-and-introduction-to-linear-regression.html#other-resources-optional-but-recommended-2"><i class="fa fa-check"></i><b>3.8</b> Other resources: optional but recommended</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="statistical-inference.html"><a href="statistical-inference.html#learning-objectives-3"><i class="fa fa-check"></i><b>4.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="4.2" data-path="statistical-inference.html"><a href="statistical-inference.html#regression"><i class="fa fa-check"></i><b>4.2</b> Regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="statistical-inference.html"><a href="statistical-inference.html#some-mathematical-notation"><i class="fa fa-check"></i><b>4.2.1</b> Some mathematical notation</a></li>
<li class="chapter" data-level="4.2.2" data-path="statistical-inference.html"><a href="statistical-inference.html#modeling-bill-depth"><i class="fa fa-check"></i><b>4.2.2</b> Modeling Bill Depth</a></li>
<li class="chapter" data-level="4.2.3" data-path="statistical-inference.html"><a href="statistical-inference.html#single-continuous-variable"><i class="fa fa-check"></i><b>4.2.3</b> Single continuous variable</a></li>
<li class="chapter" data-level="4.2.4" data-path="statistical-inference.html"><a href="statistical-inference.html#factor-and-a-continous-variable"><i class="fa fa-check"></i><b>4.2.4</b> Factor and a continous variable</a></li>
<li class="chapter" data-level="4.2.5" data-path="statistical-inference.html"><a href="statistical-inference.html#interactions"><i class="fa fa-check"></i><b>4.2.5</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="statistical-inference.html"><a href="statistical-inference.html#model-comparison-selection-and-checking-again"><i class="fa fa-check"></i><b>4.3</b> Model, comparison, selection, and checking (again)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="statistical-inference.html"><a href="statistical-inference.html#model-comparison-and-selection"><i class="fa fa-check"></i><b>4.3.1</b> Model comparison and selection</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="statistical-inference.html"><a href="statistical-inference.html#point-predictions-and-confidence-intervals"><i class="fa fa-check"></i><b>4.4</b> Point predictions and confidence intervals</a><ul>
<li class="chapter" data-level="4.4.1" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals-for-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Confidence intervals for parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="statistical-inference.html"><a href="statistical-inference.html#point-prediction"><i class="fa fa-check"></i><b>4.4.2</b> Point prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="statistical-inference.html"><a href="statistical-inference.html#tldr-lm"><i class="fa fa-check"></i><b>4.5</b> TL;DR <code>lm()</code></a><ul>
<li class="chapter" data-level="4.5.1" data-path="statistical-inference.html"><a href="statistical-inference.html#model-formula-syntax"><i class="fa fa-check"></i><b>4.5.1</b> <strong>Model formula</strong> syntax</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="statistical-inference.html"><a href="statistical-inference.html#other-resources-optional-but-recommended-3"><i class="fa fa-check"></i><b>4.6</b> Other resources: optional but recommended</a></li>
<li class="chapter" data-level="4.7" data-path="statistical-inference.html"><a href="statistical-inference.html#beyond-linear-models-to-generalised-linear-models-glms-not-examinable"><i class="fa fa-check"></i><b>4.7</b> Beyond Linear Models to Generalised Linear Models (GLMs) (<em>not examinable</em>)</a><ul>
<li class="chapter" data-level="4.7.1" data-path="statistical-inference.html"><a href="statistical-inference.html#counting-animals..."><i class="fa fa-check"></i><b>4.7.1</b> Counting animals...</a></li>
<li class="chapter" data-level="4.7.2" data-path="statistical-inference.html"><a href="statistical-inference.html#other-modelling-approaches-not-examinable"><i class="fa fa-check"></i><b>4.7.2</b> Other modelling approaches (not examinable)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html"><i class="fa fa-check"></i><b>5</b> Introduction to the design and analysis of experiments</a><ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#learning-objectives-4"><i class="fa fa-check"></i><b>5.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#key-phrases"><i class="fa fa-check"></i><b>5.2</b> Key phrases</a><ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#example-herbicide"><i class="fa fa-check"></i><b>5.2.1</b> Example: herbicide</a></li>
<li class="chapter" data-level="5.2.2" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#key-questions"><i class="fa fa-check"></i><b>5.2.2</b> Key questions:</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#three-key-principles-of-experimental-design"><i class="fa fa-check"></i><b>5.3</b> Three key principles of experimental design</a><ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#replication"><i class="fa fa-check"></i><b>5.3.1</b> <strong>Replication</strong></a></li>
<li class="chapter" data-level="5.3.2" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#randomization"><i class="fa fa-check"></i><b>5.3.2</b> <strong>Randomization</strong></a></li>
<li class="chapter" data-level="5.3.3" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#blocking"><i class="fa fa-check"></i><b>5.3.3</b> <strong>Blocking</strong></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#other-resources-optional-but-recommended-4"><i class="fa fa-check"></i><b>5.4</b> Other resources: optional but recommended</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html"><i class="fa fa-check"></i><b>6</b> Multivariate Data Methods</a><ul>
<li class="chapter" data-level="6.1" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#learning-objectives-5"><i class="fa fa-check"></i><b>6.1</b> Learning Objectives</a></li>
<li class="chapter" data-level="6.2" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#dimension-reduction"><i class="fa fa-check"></i><b>6.2</b> Dimension reduction</a><ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#pca-in-r"><i class="fa fa-check"></i><b>6.2.1</b> PCA in <code>R</code></a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#principal-components-from-the-original-variables"><i class="fa fa-check"></i><b>6.2.2</b> Principal components from the original variables</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#clustering"><i class="fa fa-check"></i><b>6.3</b> Clustering</a><ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#k-means-in-r-using-factoextra"><i class="fa fa-check"></i><b>6.3.1</b> K-means in <code>R</code> using <code>factoextra</code></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#tldr-k-means-clustering"><i class="fa fa-check"></i><b>6.4</b> TL;DR k-means clustering</a></li>
<li class="chapter" data-level="6.5" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#other-resources-optional-but-recommended-5"><i class="fa fa-check"></i><b>6.5</b> Other resources: optional but recommended</a><ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-data-methods.html"><a href="multivariate-data-methods.html#multidimensional-scaling-in-r-not-examinable"><i class="fa fa-check"></i><b>6.5.1</b> Multidimensional Scaling in <code>R</code> (<em>not examinable</em>)</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Module I BIOSCI220, University of Auckland</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate-data-methods" class="section level1">
<h1><span class="header-section-number">6</span> Multivariate Data Methods</h1>
<div id="learning-objectives-5" class="section level2">
<h2><span class="header-section-number">6.1</span> Learning Objectives</h2>
<ul>
<li>Explain the aims and motivation behind Principal Component Analysis (PCA) and its relevance in biology</li>
<li>Explain the aims and motivation behind cluster analysis and its relevance in biology</li>
<li>Write <code>R</code> code to carry out PCA</li>
<li>Interpret the effectively communicate the output of PCA</li>
<li>Assess how many principal components are needed</li>
<li>Interpret principal component scores and describe a subject with a high or low score</li>
<li>Write <code>R</code> code to carry out k-means cluster analysis</li>
<li>Interpret <code>R</code> output from PCA and k-means cluster analysis</li>
<li>Interpret and communicate, to both a statistical and non-statistical audience, multivariate data techniques, specifically,</li>
<li>PCA</li>
<li>k-means clustering</li>
</ul>
</div>
<div id="dimension-reduction" class="section level2">
<h2><span class="header-section-number">6.2</span> Dimension reduction</h2>
<p>Reduction of dimensions is needed when there are far too many features in a dataset, making it hard to distinguish between the important ones that are relevant to the output and the redundant or not-so important ones. Reducing the dimensions of data is called <strong>dimensionality reduction.</strong></p>
<p>So the aim is to <strong>find the best low-dimensional representation of the variation</strong> in a multivariate (lots and lots of variables) data set, but how do we do this?</p>
<p>One way is termed Principal Component Analysis (PCA). PCA is a <strong>feature extraction</strong> method that <strong>reduces the dimensionality of the data</strong> (number of variables) by creating new uncorrelated variables while minimizing loss of information on the original variables.</p>
<p><strong>Think of a baguette.</strong> The baguette pictured here represents two data dimensions: 1) the length of the bread and 2) the height of the bread (we'll ignore depth of bread for now). Think of the baguette as your data; when we carry out PCA we're rotating our original axes (<em>x- and y-coordinates</em>) to capture as much of the variation in our data as possible. This results in <strong>new</strong> uncorrelated variables that each explain a % of variation in our data; the procedure is designed so that the first new variable (PC1) explains the most, the second (PC2) the second most and so on.</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/cmjt/statbiscuits/master/figs_n_gifs/pca.gif" />

</div>
<p>Now rather than a baguette think of data; the baguette above represent the <em>shape</em> of the scatter between the two variables plotted below. The rotating grey axes represent the PCA procedure, essentially searching for the <em>best</em> rotation of the original axes to represent the variation in the data as best it can. Mathematically the Euclidean distance (e.g., the distance between points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> in Euclidean space, <span class="math inline">\(\sqrt{(p-q)^2}\)</span>) between the points and the rotating axes is being minimized (i.e., the shortest possible across all points), see the blue lines. Once this distance is minimized across all points we &quot;settle&quot; on our new axes (the black tiled axes).</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/cmjt/statbiscuits/master/figs_n_gifs/perp.gif" />

</div>
<p>Luckily we can do this all in <code>R</code>!</p>
<div id="pca-in-r" class="section level3">
<h3><span class="header-section-number">6.2.1</span> PCA in <code>R</code></h3>
<p><strong>Using the <code>palmerpenguins</code> data</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## we should be used to loading these packages by now :-)
<span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(palmerpenguins)
## getting rid of NAs
penguins_nafree &lt;-<span class="st"> </span>penguins <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop_na</span>()</code></pre></div>
<p>When carrying out PCA we're only interested in numeric variables, so let's just plot those. We can use the piping operator <code>%&gt;%</code> to do this with out creating a new data frame</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## introducing a new package GGally, please install
## using install.packages(&quot;GGally&quot;)
<span class="kw">library</span>(GGally)
penguins_nafree <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(species, <span class="kw">where</span>(is.numeric)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggpairs</span>(<span class="kw">aes</span>(<span class="dt">color =</span> species),
        <span class="dt">columns =</span> <span class="kw">c</span>(<span class="st">&quot;flipper_length_mm&quot;</span>, <span class="st">&quot;body_mass_g&quot;</span>, 
                     <span class="st">&quot;bill_length_mm&quot;</span>, <span class="st">&quot;bill_depth_mm&quot;</span>)) </code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p><strong>Using <code>prcomp()</code></strong></p>
<p>There are three basic types of information we obtain from Principal Component Analysis:</p>
<ul>
<li><p><strong>PC scores:</strong> the coordinates of our samples on the new PC axis: the new uncorrelated variables (stored in <code>pca$x</code>)</p></li>
<li><p><strong>Eigenvalues:</strong> (see above) represent the variance explained by each PC; we can use these to calculate the proportion of variance in the original data that each axis explains</p></li>
<li><p><strong>Variable loadings</strong> (eigenvectors): these reflect the weight that each variable has on a particular PC and can be thought of as the correlation between the PC and the original variable</p></li>
</ul>
<p>Before we carry out PCA we <strong>should</strong> scale out data. <strong>WHY?</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pca &lt;-<span class="st"> </span>penguins_nafree <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">where</span>(is.numeric), <span class="op">-</span>year) <span class="op">%&gt;%</span><span class="st"> </span>## year makes no sense here so we remove it and keep the other numeric variables
<span class="st">  </span><span class="kw">scale</span>() <span class="op">%&gt;%</span><span class="st"> </span>## scale the variables
<span class="st">  </span><span class="kw">prcomp</span>()
## print out a summary
<span class="kw">summary</span>(pca)</code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3     PC4
## Standard deviation     1.6569 0.8821 0.60716 0.32846
## Proportion of Variance 0.6863 0.1945 0.09216 0.02697
## Cumulative Proportion  0.6863 0.8809 0.97303 1.00000</code></pre>
<p>This output tells us that we obtain 4 principal components, which are called <code>PC1</code> <code>PC2</code>, <code>PC3</code>, and <code>PC4</code> (this is as expected because we used the 4 original numeric variables!). Each of these <code>PC</code>s explains a percentage of the total variation (<code>Proportion of Variance</code>) in the dataset:</p>
<ul>
<li><code>PC1</code> explains <span class="math inline">\(\sim\)</span> 68% of the total variance, which means that just over half of the information in the dataset (5 variables) can be encapsulated by just that one Principal Component.</li>
<li><code>PC2</code> explains <span class="math inline">\(\sim\)</span> 19% of the variance.</li>
<li><code>PC3</code> explains <span class="math inline">\(\sim\)</span> 9% of the variance.</li>
<li><code>PC4</code> explains <span class="math inline">\(\sim\)</span> 2% of the variance.</li>
</ul>
<p>From the <code>Cumulative Proportion</code> row we see that by knowing the position of a sample in relation to just <code>PC1</code> and <code>PC2</code> we can get a pretty accurate view on where it stands in relation to other samples, as just <code>PC1</code> and <code>PC2</code> explain 88% of the variance.</p>
<p>The <strong>loadings</strong> (<em>relationship</em>) between the initial variables and the principal components are stored in <code>pca$rotation</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pca<span class="op">$</span>rotation</code></pre></div>
<pre><code>##                          PC1         PC2        PC3        PC4
## bill_length_mm     0.4537532 -0.60019490 -0.6424951  0.1451695
## bill_depth_mm     -0.3990472 -0.79616951  0.4258004 -0.1599044
## flipper_length_mm  0.5768250 -0.00578817  0.2360952 -0.7819837
## body_mass_g        0.5496747 -0.07646366  0.5917374  0.5846861</code></pre>
<p>Here we can see that <code>bill_length_mm</code> has a strong positive relationship with <code>PC1</code>, whereas <code>bill_depth_mm</code> has a strong negative relationship. Both <code>fliper_length_mm</code> and <code>body_mass_g</code> also have a strong positive relationship with <code>PC1</code>.</p>
<p>Plotting this we get</p>
<p><img src="_main_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<p>The new variables (PCs) are stored in <code>pca$x</code>, lets plot some of them alongside the loadings using a <em>biplot</em>. For <code>PC1</code> vs <code>PC2</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(factoextra) ## install this package first
<span class="kw">fviz_pca_biplot</span>(pca, <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_point</span> (<span class="dt">alpha =</span> <span class="fl">0.2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<p>Now for <code>PC2</code> vs <code>PC3</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fviz_pca_biplot</span>(pca, <span class="dt">axes =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>),<span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>) <span class="op">+</span>
<span class="st">      </span><span class="kw">geom_point</span> (<span class="dt">alpha =</span> <span class="fl">0.2</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
<p><strong>But how many PCs (new variables) do we keep?</strong> The whole point of this exercise is to <strong>reduce</strong> the number of variables we need to explain the variation in our data. So how many of these new variables (PCs) do we keep?</p>
<p>To assess this we can use the information printed above alongside a <em>screeplot</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fviz_screeplot</span>(pca)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
</div>
<div id="principal-components-from-the-original-variables" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Principal components from the original variables</h3>
<p>Recall that the principal components are a linear combination of the (statndardised) variables. So for PC1</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loadings1 &lt;-<span class="st"> </span>pca<span class="op">$</span>rotation[,<span class="dv">1</span>]
loadings1</code></pre></div>
<pre><code>##    bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g 
##         0.4537532        -0.3990472         0.5768250         0.5496747</code></pre>
<p>Therefore, the first Principle Component will be <span class="math inline">\(0.454\times Z1 -0.399 \times Z2 + 0.5768 \times Z3 + 0.5497 \times Z3\)</span> where <span class="math inline">\(Z1\)</span>, <span class="math inline">\(Z2\)</span>, <span class="math inline">\(Z3\)</span>. and <span class="math inline">\(Z4\)</span> are the scaled numerical variables form the penguins dataset (i.e., bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g). To compute this we use <code>R</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">scaled_vars &lt;-<span class="st"> </span>penguins_nafree <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">where</span>(is.numeric), <span class="op">-</span>year) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>()
## By &quot;Hand&quot;
by_hand &lt;-<span class="st"> </span>loadings1[<span class="dv">1</span>]<span class="op">*</span>scaled_vars<span class="op">$</span><span class="st">&quot;bill_length_mm&quot;</span> <span class="op">+</span><span class="st"> </span>
<span class="st">  </span>loadings1[<span class="dv">2</span>]<span class="op">*</span>scaled_vars<span class="op">$</span><span class="st">&quot;bill_depth_mm&quot;</span> <span class="op">+</span><span class="st"> </span>
<span class="st">  </span>loadings1[<span class="dv">3</span>]<span class="op">*</span>scaled_vars<span class="op">$</span><span class="st">&quot;flipper_length_mm&quot;</span> <span class="op">+</span><span class="st"> </span>
<span class="st">  </span>loadings1[<span class="dv">4</span>]<span class="op">*</span>scaled_vars<span class="op">$</span><span class="st">&quot;body_mass_g&quot;</span>
## From PCA
pc1 &lt;-<span class="st"> </span>pca<span class="op">$</span>x[,<span class="dv">1</span>]
<span class="kw">plot</span>(by_hand,pc1)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
</div>
</div>
<div id="clustering" class="section level2">
<h2><span class="header-section-number">6.3</span> Clustering</h2>
<p>So, it's all about variation again! And the idea of minimizing it.</p>
<p>K-means clustering involves defining clusters so that the overall variation within a cluster (known as total within-cluster variation) is minimized. How do we define this variation? Typically, using Euclidean distances; the total within-cluster variation, is in this case, is defined as the sum of squared distances Euclidean distances between observations and the corresponding cluster centroid.</p>
<p>In summary, this is the procedure</p>
<ul>
<li>The number of clusters (k) are specified</li>
<li>k objects from the dataset are selected at random and <em>set</em> as the initial cluster centers or means</li>
<li>Each observation is assigned to their closest centroid (<em>based on the Euclidean distance between the object and the centroid</em>)</li>
<li>For each of the k clusters the cluster centroid is then updated based on calculating the new mean values of all the data points in the cluster</li>
<li>Repeat the two previous steps until 1) the cluster assignments stop changing or 2) the maximum number of iterations is reached</li>
</ul>
<p><strong>Identify optimal number of clusters</strong></p>
<p>Identifying the appropriate k is important because too many or too few clusters impedes viewing overall trends. Too many clusters can lead to over-fitting (which limits generalizations) while insufficient clusters limits insights into commonality of groups.</p>
<p>There are assorted methodologies to identify the appropriate <span class="math inline">\(k\)</span>. Tests range from blunt visual inspections to robust algorithms. The optimal number of clusters is ultimately a <strong>subjective decision</strong>.</p>
<div id="k-means-in-r-using-factoextra" class="section level3">
<h3><span class="header-section-number">6.3.1</span> K-means in <code>R</code> using <code>factoextra</code></h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## library for k-means clustering, will need to install first
<span class="kw">library</span>(factoextra)
df &lt;-<span class="st"> </span>penguins_nafree <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">where</span>(is.numeric), <span class="op">-</span>year)</code></pre></div>
<p>We use the <code>kmeans()</code> function from the <code>factoextra</code> package.</p>
<p>The first argument of <code>kmeans()</code> should be the dataset you wish to cluster. Below we use data frame <code>df</code>, the penguin data discussed above. But how many clusters do we choose? Let's try 1 to 5... (i.e., using the <code>centers</code> argument). Setting <code>nstart = 25</code> means that R will try 25 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## set the seed so we all start off in the same place
<span class="kw">set.seed</span>(<span class="dv">4321</span>)
## one cluster
k1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">1</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)
## two clusters
k2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">2</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)
## three clusters
k3 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)
## four clusters
k4 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">4</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)
## five clusters
k5 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">5</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</code></pre></div>
<p>The <code>kmeans()</code> function returns a list of components:</p>
<ul>
<li><code>cluster</code>, integers indicating the cluster to which each observation is allocated</li>
<li><code>centers</code>, a matrix of cluster centers/means</li>
<li><code>totss</code>, the total sum of squares</li>
<li><code>withinss</code>, within-cluster sum of squares, one component per cluster</li>
<li><code>tot.withinss</code>, total within-cluster sum of squares</li>
<li><code>betweenss</code>, between-cluster sum of squares</li>
<li><code>size</code>, number of observations in each cluster</li>
</ul>
<div id="choosing-the-number-of-clusters" class="section level4">
<h4><span class="header-section-number">6.3.1.1</span> Choosing the number of clusters</h4>
<p>We have an idea there may be 3 clusters, perhaps, but how do we know this is the best fit? Remember its a <strong>subjective choice</strong> and we'll be looking at a few pointers</p>
<p><strong>Visual inspection</strong> method</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p1 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k1, <span class="dt">data =</span> df)
p2 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k2, <span class="dt">data =</span> df)
p3 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k3, <span class="dt">data =</span> df)
p4 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k4, <span class="dt">data =</span> df)
p5 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k5, <span class="dt">data =</span> df)

## for arranging plots
<span class="kw">library</span>(patchwork) 
(p1<span class="op">|</span><span class="st"> </span>p2<span class="op">|</span><span class="st"> </span>p3)<span class="op">/</span><span class="st"> </span>(p4 <span class="op">|</span><span class="st"> </span>p5)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cluster =</span> k3<span class="op">$</span>cluster,
         <span class="dt">species =</span> penguins_nafree<span class="op">$</span>species) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(flipper_length_mm, bill_depth_mm, <span class="dt">color =</span> <span class="kw">factor</span>(cluster), <span class="dt">label =</span> species)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_text</span>()</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<p><strong>Elbow</strong> method</p>
<p>Optimal clusters are at the point in which the knee &quot;bends&quot; or in mathematical terms when the marginal total within sum of squares (<code>tot.withinss</code>) for an additional cluster begins to decrease at a linear rate</p>
<p>This is easier to see via a plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fviz_nbclust</span>(df, kmeans, <span class="dt">method =</span> <span class="st">&quot;wss&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;Elbow method&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<p>There is a pretty obvious inflection (elbow) at 2 clusters, but maybe at 3 too. We can rule out an optimal number of clusters above 3 as there is then only a minimal marginal reduction in total within sum of squares. However, the model is ambiguous on whether 2 or 3 clusters is optimal...</p>
<p><strong>Silhouette</strong> method</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Silhouette method</span>
<span class="kw">fviz_nbclust</span>(df, kmeans, <span class="dt">method =</span> <span class="st">&quot;silhouette&quot;</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;Silhouette method&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<p><strong>Gap</strong> method</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Gap statistic</span>
<span class="co"># recommended value: nboot = 500 for your analysis (it will take a while)</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>) ## remove this
<span class="kw">fviz_nbclust</span>(df, kmeans, <span class="dt">nstart =</span> <span class="dv">25</span>,  <span class="dt">method =</span> <span class="st">&quot;gap_stat&quot;</span>, <span class="dt">nboot =</span> <span class="dv">50</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;Gap statistic method&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<p><strong>Basically it's up to you to collate all the suggestions and make and informed decision</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Trying all the cluster indecies AHHHHH
<span class="kw">library</span>(NbClust)
cluster_<span class="dv">30</span>_indexes &lt;-<span class="st"> </span><span class="kw">NbClust</span>(<span class="dt">data =</span> df, <span class="dt">distance =</span> <span class="st">&quot;euclidean&quot;</span>, <span class="dt">min.nc =</span> <span class="dv">2</span>, <span class="dt">max.nc =</span> <span class="dv">9</span>, <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>, <span class="dt">index =</span><span class="st">&quot;all&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-75-1.png" width="672" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-75-2.png" width="672" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 5 proposed 2 as the best number of clusters 
## * 6 proposed 3 as the best number of clusters 
## * 1 proposed 4 as the best number of clusters 
## * 4 proposed 5 as the best number of clusters 
## * 1 proposed 8 as the best number of clusters 
## * 3 proposed 9 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  3 
##  
##  
## *******************************************************************</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fviz_nbclust</span>(cluster_<span class="dv">30</span>_indexes) <span class="op">+</span>
<span class="st">      </span><span class="kw">theme_minimal</span>() <span class="op">+</span>
<span class="st">      </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Frequency of Optimal Clusters using 30 indexes in NbClust Package&quot;</span>)</code></pre></div>
<pre><code>## Among all indices: 
## ===================
## * 2 proposed  0 as the best number of clusters
## * 1 proposed  1 as the best number of clusters
## * 5 proposed  2 as the best number of clusters
## * 6 proposed  3 as the best number of clusters
## * 1 proposed  4 as the best number of clusters
## * 4 proposed  5 as the best number of clusters
## * 1 proposed  8 as the best number of clusters
## * 3 proposed  9 as the best number of clusters
## * 3 proposed  NA&#39;s as the best number of clusters
## 
## Conclusion
## =========================
## * According to the majority rule, the best number of clusters is  3 .</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-75-3.png" width="672" /></p>
<p>Not obvious, basically still undecided between 2 and 3, but according to the absolute majority rule the &quot;best&quot; number is 3</p>
</div>
</div>
</div>
<div id="tldr-k-means-clustering" class="section level2">
<h2><span class="header-section-number">6.4</span> TL;DR k-means clustering</h2>
<p><strong><a href="https://github.com/allisonhorst/stats-illustrations">Artwork by @allison_horst</a></strong></p>
<p><img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_1.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_2.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_3.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_4.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_5.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_6.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_7.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_8.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_9.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_10.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_11.jpg" /> <img src="https://github.com/allisonhorst/stats-illustrations/raw/master/other-stats-artwork/kmeans_12.jpg" /></p>
</div>
<div id="other-resources-optional-but-recommended-5" class="section level2">
<h2><span class="header-section-number">6.5</span> Other resources: optional but recommended</h2>
<ul>
<li><p><a href="https://cmjt.github.io/statbiscuits/eigenfaces.html">Eigenfaces</a></p></li>
<li><p><a href="https://cmjt.github.io/statbiscuits/clusterducks.html">ClusterDucks</a></p></li>
<li><p><a href="https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/index.html">Little book for Multivariate Analysis</a></p></li>
<li><p><a href="https://juba.github.io/explor/">'explor' is an R package to allow interactive exploration of multivariate analysis results</a></p></li>
<li><p><a href="https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643">The Mathematics Behind Principal Component Analysis (6 min read)</a></p></li>
<li><p><a href="https://uc-r.github.io/kmeans_clustering">K-means cluster analysis</a></p></li>
</ul>
<div id="multidimensional-scaling-in-r-not-examinable" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Multidimensional Scaling in <code>R</code> (<em>not examinable</em>)</h3>
<p>Multidimensional scaling (MDS) is actually the more general technique of dimension reduction. PCA is a special case of MDS!</p>
<p>To carry out MDS in <code>R</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggfortify)
## Plotting Multidimensional Scaling (for interest)
## stats::cmdscale performs Classical MDS
<span class="kw">data</span>(<span class="st">&quot;eurodist&quot;</span>) ## road distances (in km) between 21 cities in Europe.
<span class="kw">autoplot</span>(eurodist)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Plotting Classical (Metric) Multidimensional Scaling
<span class="kw">autoplot</span>(<span class="kw">cmdscale</span>(eurodist, <span class="dt">eig =</span> <span class="ot">TRUE</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-76-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">autoplot</span>(<span class="kw">cmdscale</span>(eurodist, <span class="dt">eig =</span> <span class="ot">TRUE</span>), <span class="dt">label =</span> <span class="ot">TRUE</span>, <span class="dt">shape =</span> <span class="ot">FALSE</span>,
         <span class="dt">label.size =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-76-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Plotting Non-metric Multidimensional Scaling
## MASS::isoMDS and MASS::sammon perform Non-metric MDS
<span class="kw">library</span>(MASS)
<span class="kw">autoplot</span>(<span class="kw">sammon</span>(eurodist))</code></pre></div>
<pre><code>## Initial stress        : 0.01705
## stress after  10 iters: 0.00951, magic = 0.500
## stress after  20 iters: 0.00941, magic = 0.500</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-76-4.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">autoplot</span>(<span class="kw">sammon</span>(eurodist), <span class="dt">shape =</span> <span class="ot">FALSE</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>,<span class="dt">label.size =</span> <span class="dv">3</span>)</code></pre></div>
<pre><code>## Initial stress        : 0.01705
## stress after  10 iters: 0.00951, magic = 0.500
## stress after  20 iters: 0.00941, magic = 0.500</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-76-5.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Have a go at interpreting these plots based on the geography of the cities :-)</code></pre></div>

</div>
</div>
</div>






            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-the-design-and-analysis-of-experiments.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
