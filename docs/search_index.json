[["index.html", "Module I BIOSCI220, University of Auckland Preface 0.1 Course Overview: BIOSCI220 0.2 Module 1: Key Topics 0.3 Timetable &amp; Assessment (Modules I, II, and III)", " Module I BIOSCI220, University of Auckland Charlotte Jones-Todd Semester 1, 2021 Preface Artwork by @allison_horst 0.1 Course Overview: BIOSCI220 Living systems are the most complex things in the Universe. The science of biology is therefore the science of the complex. Other sciences, like physics and chemistry, have simpler study subjects. This may surprise students who think of other sciences as difficult and maths-heavy, and think of biology as science but with less maths. However, biological research has actually been heavily quantitative for 100+ years. Much of the development of the field of statistics was driven by and for biologists and their research problems, which usually have a large amount of natural variability. In recent decades, the computational revolution has spread to every part of biology, and all biological fields now rely heavily on analyses that would be impossible without computers and computer programming: &quot;big data&quot; studies and complex models of biological phenomena. Therefore, in order to understand modern biological research and findings, and to participate in this research (and get jobs!), it is now essential for biology students to acquire skills in working with and visualising data, learning from data using models, and generating data using simulations of models. These might be classic statistical models, simulation models, or inference with process-based models. Most importantly, students need to gain the ability to be careful and critical thinkers about data and how it is acquired, as well as the ability to think critically about the models that we use to try to simplify, and thereby understand, the incredible complexity of biology. 0.2 Module 1: Key Topics Data Exploration and Statistical Inference Data wrangling and visualisation. Introduction to R, importing and plotting data, R packages Experimental design and introduction to linear models Linear models with multiple variables; model interpretation Interpretation of p-values; model critique and model comparison Large data, exploratory data analysis, introduction to clustering and dimensionality reduction Slides here 0.3 Timetable &amp; Assessment (Modules I, II, and III) Lectures Monday 9‚Äì10am Labs Monday 2‚Äì5pm Tuesday 10am‚Äì1pm Thursday 10am‚Äì1pm Friday 10am‚Äì1pm Lab exercises 60% Total (11 labs in Total; 6 for Module 1, weeks 1--6) Due weekly by 5pm Weekly quizzes 10% Total (11 quizzes in Total; 5 for Module 1, weeks 2--6) Due weekly by 5pm Final Exam 30% "],["r-and-rstudio.html", "1 R and RStudio 1.1 Learning Objectives 1.2 Introduction to R and RStudio 1.3 Exploratory Data Analysis (EDA) 1.4 Other resources: optional but recommended", " 1 R and RStudio The purpose of this chapter is to get you started learning a new language! Throughout BIOSCI220 you will be introduced to tools required to critically analyse and interpret biological data. Throughout this module you will be expected to use R and RStudio weekly. It is highly recommended that you familiarise yourself with these environments using the computer on which you plan to carry out the majority of your work. If this is a lab computer then R and RStudio will already been installed. If you choose to use these then you should still complete the exercises below to familiarise yourself with the software. Another option available is the use of RStudio Cloud; here, everything is run in a web browser (on a remote server) and doesn't require you to download the software onto your personal computer. However, if you plan to use your personal computer then you will need to install both R and RStudio. Follow the directions in Installing R and RStudio to do so. 1.1 Learning Objectives Define the difference between R and RStudio Express the benefits and issues associated with these software being used in the scientific community. Specifically, summarise the benefits and drawbacks associated with the open-source paradigm, discuss the concept of reproducible research and outline its importance Distinguish between different data types (e.g., integers, characters, logical, numerical) Explain what an R function is; describe what an argument to an R function is Explain what an R package is; distinguish between the functions install.packages() and library() Explain what a working directory is in the context of R Interpret and fix basic R errors. For example ## Error in library(fiddler): there is no package called &#39;fiddler&#39; and ## Warning in file(file, &quot;rt&quot;): cannot open file &#39;paua.csv&#39;: No such file or ## directory ## Error in file(file, &quot;rt&quot;): cannot open the connection Use the appropriate R function to read in a .csv data; carry out basic exploratory data analysis using tidyverse (use the pipe operator, %&gt;% when summarising a data.frame); create simple plots of the data. 1.2 Introduction to R and RStudio R is the pheromone to RStudio's PDA. R is a language, specifically, a programming language; it's the way you can speak to your computer to ask it to carry out certain computations. RStudio is an integrated development environment (IDE). This means it is basically an interface, albeit a fancy one, that makes it easier to communicate with your computer in the language R. The main benefit is the additional features it has that enable you to more efficiently speak R. Note R and RStudio are two different pieces of software; for this course you are expected to download both. As you'd expect, the PDA depends on the pheromones (i.e., RStudio depends on R) so you have to download R to use RStudio! 1.2.1 Why? The selling pitch of this course states that ...biological research has actually been heavily quantitative for 100+ years... and promises that ...it is now essential for biology students to acquire skills in working with and visualising data, learning from data using models.... We're not making it up! If you need convincing that quantitative and programming skills are essential to graduate in all scientific disciplines have a read of the following. The Popularity of Data Science Software Why R? 1.2.1.1 WhyR? It's free It's open source A general-purpose of programming language Written by statisticians (here in Auckland!) It's available for all operating systems (Windows, Linux, and Mac) There is a huge online support network It's extremely flexible; if you can code it you can do it! 15,000+ packages available! ... 1.2.1.2 Why RStudio? &quot;If R were an airplane, RStudio would be the airport...&quot; --- Julie Lowndes, Introduction to RStudio Awesomeness Speaks nicely to R Tab completion Debugging capabilities There is a huge online support network Offers many other features and tools to make your workflow with R easier It facilitates reproducibility ... 1.2.2 Installing R and RStudio As mentioned above RStudio depends on R so there is an order you should follow when you download these software. Download and install R by following these instructions. Make sure you choose the correct operating system; if you are unsure then please ask either a TA or myself. Download and install RStudio by going here choosing RStudio Desktop Open Source License Free and following instructions. Again if you are unsure then please ask either a TA or myself. Check all is working Open up RStudio from your computer menu, the icon will look something like this (DO NOT use this icon , this is a link to R and will only open a very basic interface) Wait a little and you should see RStudio open up to something similar to the screenshot below Pay close attention to the notes in the screenshot and familiarise yourself with the terms. Finally, in the Console next to the prompt type 1:10 and press enter on your keyboard. Your computer should say something back you (in the Console)! What do you think you were asking it to do? Does the output make sense?1 If you get stuck at any of the steps above then please ask either a TA or myself. It is imperative for the rest of the course that you complete the steps above. 1.2.2.1 Good practice* Always start with a clean workspace Why? *So your ex (code) can't come and mess up your life!** Go to Tools &gt; Global Options Project-oriented workflow. Recommended: .Rproj Organised Set up each Each assignment/university course as a project Self-contained a project is a folder that contains all relevant files All paths can then be relative to that project Reproducible the project should just work on a different computer Got to Project (top right) &gt; New Project &gt; Create Project Project set-up ‚ö†Ô∏èWarning‚ö†Ô∏è Jenny Bryan will set your computer on fire üî• if you start your script like this rm(list = ls()) This does NOT create a fresh R process it makes your script vulnerable it will come back to bite you 1.2.3 Getting started As in step 3. above open up RStudio from your computer menu, the icon will look something like this . Using the diagram above identify the different panes: Console where you directly type command in and communicate with your computer (via the language R). Environment pane Files pane Some terminology Running code: the act of telling R to perform an act by giving it commands in the console. Objects: where values are saved in (see later for creating an object. Script: a text file containing a set of commands and comments. Comments: notes written within a Script to better document/explain what's happening 1.2.4 R Scripts (a .r file) Go File &gt; New File &gt; R Script to open up a new Script If you had only three panes showing before, a new (fourth) pane should open up in the top left of RStudio. This file will have a .r extension and is where you can write, edit, and save the R commands you write. It's a dedicated text editor for your R code (very useful if you want to save your code to run at a later date). The main difference between typing your code into a Script vs Console is that you edit it and save it for later! Remember though the Console is the pane where you communicate with your computer so all code you write will have to be Run here. There are two ways of running a line of code you've written in your Script Ensure your cursor is on the line of code you want to run, hold down Ctrl and press Enter. Ensure your cursor is on the line of code you want to run, then use your mouse to click the Run button (it has a green arrow next to it) on the top right of the Script pane. Type 1:10 in your Script and practise running this line of code using both methods above. Not that if you've Run the code successfully then your computer will speak back to you each time via the Console 1.2.5 Writing Comments Comments are notes to yourself (future or present) or to someone else and are, typically, written interspersed in your code. Now, the comments you write will typically be in a language your computer doesn't understand (e.g., English). So that you can write yourself notes in your Script you need to tell your computer using the R language to ignore them. To do this precede any note you write with #, see below. The # is R for ignore anything after this character. ## IGNORE ME ## I&#39;m a comment ## I repeat I&#39;m a comment ## I am not a cat ## OK let&#39;s run some code 2 + 2 ## [1] 4 ## Hmm maybe I should check this ## @kareem_carr ;-) Now remember when you want to leave your R session you'll need to Save your Script to use it again. To do this go File &gt; Save As and name your file what you wish (remember too to choose a relevant folder on your computer, or as recommended use the .Rproj set-up as above). 1.2.6 Data types Artwork by @allison_horst Here we're covering data types in R (e.g., integers, doubles/numeric, logical, and characters). Integers are whole values like 1, 0, 220. These are classified &quot;integer&quot; or int in R. Numeric values are a larger set of values containing integers but also fractions and decimal values, for example -56.94 and 1.3. These are classified &quot;numeric&quot; or num or dbl in R. Logicals are either TRUE or FALSE. These are classified &quot;logical&quot; or lgl in R. Characters are text such as ‚ÄúCharlotte‚Äù, ‚ÄúBIOSCI220‚Äù, and ‚ÄúStatistics is the greatest subject ever‚Äù. Note that characters are denoted with the quotation marks around them and are classified &quot;character&quot; or chr in R. ## As an example we&#39;re going to as our computer using R what it classified the character string &quot;Charlotte&quot; as class(&quot;Charlotte&quot;) ## [1] &quot;character&quot; 1.2.7 Creating Objects Objects are created values using the symbols &lt;- (an arrow formed out of &lt; and -). Like we, typically, write an equation the left-hand side is the Object we're defining (creating) and the right-hand side is the stuff we're defining it as. For example, below I'm creating the Object my_name and assigning it the character string of my first name. my_name &lt;- &quot;Charlotte&quot; So now the Object my_name ‚Äòcontains‚Äô the value &quot;Charlotte&quot;. Another assignment to the same object will overwrite the content. my_name &lt;- &quot;Moragh&quot; To check the content of an Object you can simply as your computer to print it out for you (in R). my_name ## [1] &quot;Moragh&quot; Note: R is case sensitive: it treats my_name and My_Name as different objects. An object can be assigned a collection of things: my_names &lt;- c(&quot;Charlotte&quot;, &quot;Moragh&quot;, &quot;Jones-Todd&quot;) my_names ## [1] &quot;Charlotte&quot; &quot;Moragh&quot; &quot;Jones-Todd&quot; some_numbers &lt;- c(1,4,5,13,45,90) some_numbers ## [1] 1 4 5 13 45 90 An Object can also be an entire dataset, see Exploratory Data Analysis (EDA) below. 1.2.8 R functions Functions (or commands) perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a function‚Äôs arguments or use the function‚Äôs default values. For example, the function seq() in R generates a sequence of numbers. If you just run seq() it will return the value 1. That doesn‚Äôt seem very useful! This is because the default arguments are set as seq(from = 1, to = 1). Thus, if you don't pass in different values for from and to to change this behaviour, your computer just assumes all you want is the number 1. You can change the argument values by updating the values after the = sign. If we try out seq(from = 2, to = 5) we get the result rseq(from = 2, to = 5)` that we might expect. 1.2.9 R packages An R package is simply a collection of functions! Typically all focused on a particular type of procedure. The base installation of R comes with many useful packages as standard and these packages will contain many of the functions you will use on a daily basis (e.g., mean(), length()). However, often we wish to do more than base R offer! To do this we need to access all the other amazing packages there are in the Rverse. CRAN is like a centralised library with thousands of books in stock. To access the contents of a book (package) you first need to request it for (install it into) your local library (your computer) Your can only access books in your local library. install.packages(&#39;the.package.name&#39;) To access the knowledge in a particular book (use the function is the package) you need to tell your computer via R to go get the book of the shelf. Then you have access to all the functions it contains! library(the.package.name) 1.2.10 R Errors Sometimes rather than doing what you expect it to your computer will return an Error message to you via the Console prefaced with **Error in...* followed by text that will try to explain what went wrong. This, generally, means something has gone wrong, so what do you do? Read it! THE MESSAGES ARE WRITTEN IN PLAIN ENGLISH (MOSTLY) DO NOT continue running bits of code hoping the issue will go away. IT WILL NOT. Try and work out what it means and fix it by: reading the documentation, see [] search or ask questions on Stack Overflow or RStudio Community Sometimes your computer will return a warning messages to you prefaced &quot;Warning:&quot;. These can sometimes be ignored as they may not affect us. However, READ THE MESSAGE and decide for yourself. Occasionally, also your computer will write you a friendly message, just keeping you up-to date with what it's doing, again don't ignore these they might be telling you something useful! 1.2.11 Working directories You need to tell your computer where to look! Look at the top of your Console. You will see something like ~/Desktop/ or C://Users/... (it won't be an exact match of course). This is the 'address' of where your computer is looking. Now, run getwd() and see what output you get (it will be the same as written on the top of your Console pane. This is because getwd() stands for get the current workingdirectory (i.e., the current directory you are currently working in) e.g., getwd() ## [1] &quot;/home/charlotte/Git/BIOSCI220/docs&quot; You should ensure that you are aware of which directory you're working in (which folder RStudio is looking in by default) as this is important later on when we come to reading in files and saving our work! 1.2.11.1 Changing address So you're not where you want to be! Click Session &gt; Set Working Directory &gt; Choose Directory &gt; Chose where you want to go Now notice that something has been written in your Console something similar to setwd(&quot;~/Git/BIOSCI220/data&quot;). Now setwd() stands for set your workingdirectory. If you know the address of the directory you want to work in without having to point-and-click you could use this command directly, in this case you've used the point-and-click to do it and RStudio has helpfully written out your choices as an R command. 1.2.12 Getting help [Where to go?] Let's say we want to learn more about the function mean() (we can take a wild guess at what it calculates, but... what if we didn't know for sure. There are two ways we can ask within RStudio ?mean() or help(mean) Try both and see what pops up in your right-hand bottom pane! Failing that Google is (often) your friend 1.3 Exploratory Data Analysis (EDA) To finish off this section and to ensure you're all set to go for the rest of the module we're going to carry out some exploratory data analysis and visualisations on some real-world data :-) For this module we will be using tidyverse. 'tidyverse' is a collection of R packages that all share underlying design philosophy, grammar, and data structures. They are specifically designed to make data wrangling, manipulation, visualisation, and analysis simpler. To install all the packages that belong to the tidyverse run ## request (download) the tidyverse packages from the centralised library install.packages(&quot;tidyverse&quot;) To tell your computer to access the tidyverse functionality in your session run (Note you'll have to do this each time you start up an R session): ## Get the tidyverse packages from our local library ## Do this every time you start a new session ## and want to use the tidyverse! library(tidyverse) 1.3.1 Reading in data from a .csv file First off download the paua.csv file from CANVAS onto your computer (remember which folder you saved it in!) To read the data into RStudio In the Environment pane click Import Dataset &gt; ** From Text (readr)** &gt; Browse &gt; Choose your file, remembering which folder you downloaded it to &gt; Another pane should pop up, check the data looks as you might expect &gt; Import You should now notice that in the Environment pane there is something listed under Data (this is the name of the data.frame Object containing the data we will explore) Now notice how in the Console a few lines of code have been added. These are the commands you were telling your computer via the point-and-click procedure you went through! Notice the character string inside read_csv()... This is the full 'address' of your data (the folder you saved it in). When you tell your computer to look for something you need to tell it exactly where it is! Remember the getwd() command above, this tells you the default location RStudio will look for a file, if your file is not in this folder you have to tell it the full address. 1.3.2 Using functions to explore the data Automatically RStudio has run the command View() for you. This makes your dataset show itself in the top left pane. It's like looking at the data in Excel. Follow along with the commands below, I recommend that you open up a new Script and use that to write and save your commands for later. Don't forget to ensure you have read the paua into your session (all commands below assume that your data Object is called paua, if you've called it something different then just replace paua with whatever you've called it below. Now let's go ahead and use some functions to ask and answer questions about our data. The first thing you should always do is view any data frames you import. Let's have a look at your data in the Console paua ## # A tibble: 60 x 3 ## Species Length Age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Haliotis iris 1.8 1.50 ## 2 Haliotis australis 5.4 11.9 ## 3 Haliotis australis 4.8 5.42 ## 4 Haliotis iris 5.75 4.50 ## 5 Haliotis iris 5.65 5.50 ## 6 Haliotis iris 2.8 2.50 ## 7 Haliotis australis 5.9 6.49 ## 8 Haliotis iris 3.75 5.00 ## 9 Haliotis australis 7.2 8.56 ## 10 Haliotis iris 4.25 5.50 ## # ‚Ä¶ with 50 more rows So, what does this show us? A tibble: 60 x 3: A tibble is a specific kind of data frame in R. Our paua dataset has 60 rows (i.e., 60 different observations). Here, each observation corresponds to a P\\(\\overline{a}\\)ua shell. 3 columns corresponding to 3 variables describing each observation. Species, Length, and Age are the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 P\\(\\overline{a}\\)uashells. ``... with 50 more rows indicates there are 50 more rows to see, but these have not been printed (likely as it would clog our screen) Let's look at some other ways of exploring the data. Using the View() command (recall from above) to explore the data in a pop-up viewer View(paua) Using the glimpse() command for an alternative view glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, ‚Ä¶ ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6‚Ä¶ ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, ‚Ä¶ glimpse() will give you the first few entries of each variable in a row after the variable name. Note also, that the data type of the variable is given immediately after each variables name inside &lt; &gt;. 1.3.2.1 The pipe operator %&gt;% A nifty tidyverse tool is called the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): This is where the pipe operator %&gt;% comes in handy. %&gt;% takes the output of one function and then ‚Äúpipes‚Äù it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as ‚Äúthen‚Äù or ‚Äúand then.‚Äù For example, you can obtain the same output as the hypothetical sequence of functions as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So to calculate the mean Age of each Species in the paua dataset we would use paua %&gt;% group_by(Species) %&gt;% summarize(mean_age = mean(Age)) ## # A tibble: 2 x 2 ## Species mean_age ## * &lt;chr&gt; &lt;dbl&gt; ## 1 Haliotis australis 7.55 ## 2 Haliotis iris 4.40 You would read the sequence above as: Take the paua data.frame then Use this and apply the group_by() function to group by Species Use this output and apply the summarize() function to calculate the mean (using (mean()) Age of each group (Species), calling the resulting number mean_age 1.3.3 Basic plotting (for your own purposes) The payoff is so clear: you make informative plots that help you understand data. boxplot(Age ~ Species, data = paua) So what have we asked our computer to do here? Given what we know about the types of data our paua Object contains what plots do you think would be most appropriate for each variable? Below is some example code; each line of code will produce a plot (perhaps not a sensible one though). What do you think? What is each plot showing us? boxplot(Length ~ Species, data = paua) boxplot(Age ~ Species, data = paua) plot(Age ~ Length, data = paua) boxplot(Age ~ Length, data = paua) plot(paua$Age) 1.4 Other resources: optional but recommended Artwork by @allison_horst R for Data Science RStudio Education An Introduction to R Learning statistics with R: A tutorial for psychology students and other beginners R for Biologists Quantitative Biology: Basic Introduction to R You should have seen the numbers 1 to 10 printed out as a sequence.‚Ü© "],["data-exploration-and-visualization.html", "2 Data exploration and visualization 2.1 Learning objectives 2.2 Data sovereignty 2.3 Data wrangling and manipulation 2.4 Data Viz 2.5 Other resources: optional but recommended", " 2 Data exploration and visualization So now you've been introduced to R and RStudio let's get going with some data manipulation and visualization. Exploring and visualising your data is one of the most important steps. It's also one of the simplest! You'll not find anyone who's not made the mistake of taking their data for granted. Just because someone says it's so NEVER trust that that's the case. From typos, to NAs, through -999 and let's not even talk dates, your data will always have a surprise in store for you. 2.1 Learning objectives Define and discuss MƒÅori Data Sovereignty principles Define data sovereignty and explain this in relation to a researcher's obligation when collecting, displaying, and analysing data Carry out and interpret the outputs of basic exploratory data analysis using in-built R functions Create and communicate informative data visualisations using R Discuss and critique data visualisations 2.2 Data sovereignty ''Data sovereignty is the idea that data are subject to the laws and governance structures within the nation it is collected'' &quot;For Indigenous peoples, historical encounters with statistics have been fraught, and none more so than when involving official data produced as part of colonial attempts at statecraft.&quot; --- Lovett, R., Lee, V., Kukutai, T., Cormack, D., Rainie, S.C. and Walker, J., 2019. Good data practices for Indigenous data sovereignty and governance. Good data, pp.26-36. 2.2.1 MƒÅori Data Sovereignty principles &quot;MƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.&quot; --- Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy ‚ÄúMƒÅori data refers to data produced by MƒÅori or that is about MƒÅori and the environments we have relationships with.&quot; --- Te Mana Raraunga Charter Data is a ‚Äúpotential taonga, something precious that needs to be maintained, in relation to its utility‚Äù --- Dr W. Edwards, TMR website Article Two of the Treaty of Waitangi obliges the Government to actively protect [taonga])(https://maoridictionary.co.nz/search?keywords=taonga), consult with MƒÅori in respect of taonga, give effect to the principle of partnership and recognize MƒÅori rangatiratanga over taonga. Factors that relate to how communities might recognize the taonga nature of any dataset include provenance of the data: does the dataset come from a significant MƒÅori source? opportunity for the data: could the dataset support MƒÅori aspirations for their people or their whenua (land)? utility of the data: does the dataset have multiple uses? MƒÅori Data Sovereignty principles inform the recognition of MƒÅori rights and interests in data, and promote the ethical use of data to enhance MƒÅori wellbeing: Rangatiratanga (authority) MƒÅori have an inherent right to exercise control over MƒÅori data and MƒÅori data ecosystems. This right includes, but is not limited to, the creation, collection, access, analysis, interpretation, management, security, dissemination, use and reuse of MƒÅori data. Decisions about the physical and virtual storage of MƒÅori data shall enhance control for current and future generations. Whenever possible, MƒÅori data shall be stored in Aotearoa New Zealand. MƒÅori have the right to data that is relevant and empowers sustainable self-determination and effective self-governance Whakapapa (relationships) All data has a whakapapa (genealogy). Accurate metadata should, at minimum, provide information about the provenance of the data, the purpose(s) for its collection, the context of its collection, and the parties involved. The ability to disaggregate MƒÅori data increases its relevance for MƒÅori communities and iwi. MƒÅori data shall be collected and coded using categories that prioritise MƒÅori needs and aspirations. Current decision-making over data can have long-term consequences, good and bad, for future generations of MƒÅori. A key goal of MƒÅori data governance should be to protect against future harm. Whanaungatanga (obligations) Individuals‚Äô rights (including privacy rights), risks and benefits in relation to data need to be balanced with those of the groups of which they are a part. In some contexts, collective MƒÅori rights will prevail over those of individuals. Individuals and organisations responsible for the creation, collection, analysis, management, access, security or dissemination of MƒÅori data are accountable to the communities, groups and individuals from whom the data derive Kotahitanga (collective benefit) Data ecosystems shall be designed and function in ways that enable MƒÅori to derive individual and collective benefit. Build capacity. MƒÅori Data Sovereignty requires the development of a MƒÅori workforce to enable the creation, collection, management, security, governance and application of data. Connections between MƒÅori and other Indigenous peoples shall be supported to enable the sharing of strategies, resources and ideas in relation to data, and the attainment of common goals. Manaakitanga (reciprocity) The collection, use and interpretation of data shall uphold the dignity of MƒÅori communities, groups and individuals. Data analysis that stigmatises or blames MƒÅori can result in collective and individual harm and should be actively avoided. Free, prior and informed consent shall underpin the collection and use of all data from or about MƒÅori. Less defined types of consent shall be balanced by stronger governance arrangements. Kaitiakitanga (guardianship) MƒÅori data shall be stored and transferred in such a way that it enables and reinforces the capacity of MƒÅori to exercise kaitiakitanga over MƒÅori data. Ethics. Tikanga, kawa (protocols) and mƒÅtauranga (knowledge) shall underpin the protection, access and use of MƒÅori data. MƒÅori shall decide which MƒÅori data shall be controlled (tapu) or open (noa) access. The Te Mana o te Raraunga Model was developed to align MƒÅori concepts with data rights and interests, and guide agencies in the appropriate use of MƒÅori data Whakapapa and whanaungatanga: recognising the connectedness between the material, natural and spiritual worlds Rangatiratanga: MƒÅori rights to own, access, control and possess data from them or about them and their environs Kotahitanga: collective vision and unity of purpose Manaakitanga: ethical data use to progress MƒÅori aspirations for wellbeing Kaitiakitanga: sustainable data stewardship Resources Lovett, R., Lee, V., Kukutai, T., Cormack, D., Rainie, S.C. and Walker, J., 2019. Good data practices for Indigenous data sovereignty and governance. Good data, pp.26-36. Walter, Maggie, Tahu Kukutai, Stephanie Russo Carroll, and Desi Rodriguez-Lonebear. Indigenous Data Sovereignty and Policy. Taylor &amp; Francis, 2020. 2.3 Data wrangling and manipulation tidy data &quot;Tidy datasets are all alike, but every messy dataset is messy in its own way.&quot; --- Hadley Wickham There are three interrelated rules which make a dataset tidy: Each variable must have its own column Each observation must have its own row Each value must have its own cell Illustration from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst Why ensure that your data is tidy? Consistency: using a consistent format aids learning and reproducibility Simplicity: it's a format that is well understood by R &quot;Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets.&quot; --- Hadley Wickham, Tidy data Tidy Data 2.3.1 Introuducing the Palmer penguins library(palmerpenguins) ## contains some nice penguin data penguins ## # A tibble: 344 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_‚Ä¶ body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torge‚Ä¶ 39.1 18.7 181 3750 ## 2 Adelie Torge‚Ä¶ 39.5 17.4 186 3800 ## 3 Adelie Torge‚Ä¶ 40.3 18 195 3250 ## 4 Adelie Torge‚Ä¶ NA NA NA NA ## 5 Adelie Torge‚Ä¶ 36.7 19.3 193 3450 ## 6 Adelie Torge‚Ä¶ 39.3 20.6 190 3650 ## 7 Adelie Torge‚Ä¶ 38.9 17.8 181 3625 ## 8 Adelie Torge‚Ä¶ 39.2 19.6 195 4675 ## 9 Adelie Torge‚Ä¶ 34.1 18.1 193 3475 ## 10 Adelie Torge‚Ä¶ 42 20.2 190 4250 ## # ‚Ä¶ with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; So, what does this show us? A tibble: 344 x 8: A tibble is a specific kind of data frame in R. The penguin dataset has 344 rows (i.e., 344 different observations). Here, each observation corresponds to a penguin. 8 columns corresponding to 3 variables describing each observation. species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year are the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 penguins. ``... with 334 more rows indicates there are 334 more rows to see, but these have not been printed (likely as it would clog our screen) To learn more about the penguins read the paper that talks all about the data collection. 2.3.2 Common dataframe manipulations in the tidyverse, using dplyr and tidyr Even from these first few rows of data we can see that there are some NA values. Let's count the number of NAs. Remember the %&gt;% operator? Here we're going to be introduced to a few new things the apply() function, the is.na() function, and how R deals with logical values! library(tidyverse) penguins %&gt;% apply(.,2,is.na) %&gt;% apply(.,2,sum) ## species island bill_length_mm bill_depth_mm ## 0 0 2 2 ## flipper_length_mm body_mass_g sex year ## 2 2 11 0 There's lot going on in that code! Let's break it down Take penguins then Use penguins as an input to the apply() function (this is specified as the first argument using the .) Now the apply() function takes 3 arguments: the data object you want it to apply something to (in our case penguins) the margin you want to apply that something to; 1 stands for rows and 2 stands for columns, and the function you want it to apply (in our case is.na()). So the second line of code is asking R to apply the is.na() function over the columns of penguins is.na() asks for each value it's fed is it an NA value; it returns a TRUE if so and a FALSE otherwise The output from the first apply() is then fed to the second apply() (using the .). The sum() function then add them up! R treats a TRUE as a 1 and a FALSE as a 0. So how many NAs do you think there are! Doesn't help much. To Now we know there are NA values throughout the data let's remove then and create a new NA free version called penguins_nafree. There is a really handy tidyverse (dplyr) function for this! penguins_nafree &lt;- penguins %&gt;% drop_na() penguins_nafree ## # A tibble: 333 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_‚Ä¶ body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torge‚Ä¶ 39.1 18.7 181 3750 ## 2 Adelie Torge‚Ä¶ 39.5 17.4 186 3800 ## 3 Adelie Torge‚Ä¶ 40.3 18 195 3250 ## 4 Adelie Torge‚Ä¶ 36.7 19.3 193 3450 ## 5 Adelie Torge‚Ä¶ 39.3 20.6 190 3650 ## 6 Adelie Torge‚Ä¶ 38.9 17.8 181 3625 ## 7 Adelie Torge‚Ä¶ 39.2 19.6 195 4675 ## 8 Adelie Torge‚Ä¶ 41.1 17.6 182 3200 ## 9 Adelie Torge‚Ä¶ 38.6 21.2 191 3800 ## 10 Adelie Torge‚Ä¶ 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Below are some other useful manipulation functions; have a look at the outputs and run them yourselves and see if you can work out what they're doing. filter(penguins_nafree, island == &quot;Torgersen&quot; ) ## # A tibble: 47 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_‚Ä¶ body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torge‚Ä¶ 39.1 18.7 181 3750 ## 2 Adelie Torge‚Ä¶ 39.5 17.4 186 3800 ## 3 Adelie Torge‚Ä¶ 40.3 18 195 3250 ## 4 Adelie Torge‚Ä¶ 36.7 19.3 193 3450 ## 5 Adelie Torge‚Ä¶ 39.3 20.6 190 3650 ## 6 Adelie Torge‚Ä¶ 38.9 17.8 181 3625 ## 7 Adelie Torge‚Ä¶ 39.2 19.6 195 4675 ## 8 Adelie Torge‚Ä¶ 41.1 17.6 182 3200 ## 9 Adelie Torge‚Ä¶ 38.6 21.2 191 3800 ## 10 Adelie Torge‚Ä¶ 34.6 21.1 198 4400 ## # ‚Ä¶ with 37 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; summarise(penguins_nafree, avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 1 x 1 ## avgerage_bill_length ## &lt;dbl&gt; ## 1 44.0 group_by(penguins_nafree, species) ## # A tibble: 333 x 8 ## # Groups: species [3] ## species island bill_length_mm bill_depth_mm flipper_length_‚Ä¶ body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torge‚Ä¶ 39.1 18.7 181 3750 ## 2 Adelie Torge‚Ä¶ 39.5 17.4 186 3800 ## 3 Adelie Torge‚Ä¶ 40.3 18 195 3250 ## 4 Adelie Torge‚Ä¶ 36.7 19.3 193 3450 ## 5 Adelie Torge‚Ä¶ 39.3 20.6 190 3650 ## 6 Adelie Torge‚Ä¶ 38.9 17.8 181 3625 ## 7 Adelie Torge‚Ä¶ 39.2 19.6 195 4675 ## 8 Adelie Torge‚Ä¶ 41.1 17.6 182 3200 ## 9 Adelie Torge‚Ä¶ 38.6 21.2 191 3800 ## 10 Adelie Torge‚Ä¶ 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Often we want to summarise variables by different groups (factors). Below we Take the penguins_nafree data then Use this and apply the group_by() function to group by species Use this output and apply the summarize() function to calculate the mean (using (mean()) bill length (bill_length_mm) of each group (species), calling the resulting number avgerage_bill_length penguins_nafree %&gt;% group_by(species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 3 x 2 ## species avgerage_bill_length ## * &lt;fct&gt; &lt;dbl&gt; ## 1 Adelie 38.8 ## 2 Chinstrap 48.8 ## 3 Gentoo 47.6 We can also group by multiple factors, for example, penguins_nafree %&gt;% group_by(island,species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## `summarise()` has grouped output by &#39;island&#39;. You can override using the `.groups` argument. ## # A tibble: 5 x 3 ## # Groups: island [3] ## island species avgerage_bill_length ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Biscoe Adelie 39.0 ## 2 Biscoe Gentoo 47.6 ## 3 Dream Adelie 38.5 ## 4 Dream Chinstrap 48.8 ## 5 Torgersen Adelie 39.0 2.4 Data Viz &quot;...have obligations in that we have a great deal of power over how people ultimately make use of data, both in the patterns they see and the conclusions they draw.&quot; --- Michael Correll, Ethical Dimensions of Visualization Research &quot;Clutter and confusion are not attributes of data - they are shortcomings of design.&quot; --- Edward Tufte 2.4.1 Exploratory and explanatory plots Exploratory plots (for you) data exploration doesn't have to look pretty just needs to get to the point explore and discover new data facets formulate new questions For example, Explanatory plots (for others), most common kind of graph used in scientific publications clear purpose designed for the audience make it easy to read (this covers a lot of things) do not distort guide the reader to a particular conclusion answer a specific question support a decision For example, Plots by Cedric Scherer and mentioned on this blog 2.4.2 Ten Simple Rules for Better Figures &quot;Scientific visualization is classically defined as the process of graphically displaying scientific data. However, this process is far from direct or automatic. There are so many different ways to represent the same data: scatter plots, linear plots, bar plots, and pie charts, to name just a few. Furthermore, the same data, using the same type of plot, may be perceived very differently depending on who is looking at the figure. A more accurate definition for scientific visualization would be a graphical interface between people and data.&quot; --- Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Know Your Audience Identify Your Message Adapt the Figure to the Support Medium Captions Are Not Optional Do Not Trust the Defaults Use Color Effectively Do Not Mislead the Reader There are formulas to measure how misleading a graph is! Avoid Chartjunk Message Trumps Beauty &quot;message and readability of the figure is the most important aspect while beauty is only an option&quot; --- Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Get the Right Tool I'm an advocate for R üòâ So we've seen some pretty plots, let's get around to making some! 2.4.3 Introducing ggplot2 ggplot2 is an R package for producing statistical, or data, graphics; it has an underlying grammar based on the Grammar of Graphics Every ggplot2 plot has three key components: data, A set of aesthetic mappings between variables in the data and visual properties, and At least one layer which describes how to render each observation. Layers are usually created with a geom function. 2.4.3.1 Plotting palmerpenguins You might find this application useful, now and later... We've seen that there are three factor variables in the dataset: species, island, and sex. To count the number of penguins of each species and sex on each island we could use penguins_nafree %&gt;% count(species, sex, island) ## # A tibble: 10 x 4 ## species sex island n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Adelie female Biscoe 22 ## 2 Adelie female Dream 27 ## 3 Adelie female Torgersen 24 ## 4 Adelie male Biscoe 22 ## 5 Adelie male Dream 28 ## 6 Adelie male Torgersen 23 ## 7 Chinstrap female Dream 34 ## 8 Chinstrap male Dream 34 ## 9 Gentoo female Biscoe 58 ## 10 Gentoo male Biscoe 61 It's not too easy to compare the numbers here; what about a bar graph (geom_bar())? Based on what we went through in the lecture see if you can figure out what each line is adding to the plot. What do you think facet_wrap() is doing? To figure it out run the code yourself and try changing some of the lines of code. ggplot(penguins_nafree, aes(x = species, fill = sex)) + geom_bar(alpha = 0.8, position = &quot;dodge&quot;) + facet_wrap(~island) + xlab(&quot;&quot;) + theme_linedraw() + ## remember themes... scale_fill_manual(values = c(&quot;cyan4&quot;,&quot;darkorange&quot;), name = &quot;Sex&quot;) We also saw there were a few continuous variables, so let's look at scatter plots (geom_point()). ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point(aes(color = species),size = 2) + scale_color_manual(values = c(&quot;darkorange&quot;,&quot;darkorchid&quot;,&quot;cyan4&quot;), name = &quot;&quot;) + theme_bw() + ## Oo a new theme xlab(&quot;Bill length (mm)&quot;) + ylab(&quot;Bill length (mm)&quot;) What about the spread/distribution of our continuous variables by the factor variables (e.g., species): Boxplots (geom_boxplot())? Violin plots (geom_violin())? Histograms (geom_histogram())? In addition,we should always avoid using similarly bight red and green colours: they may not be distinguishable for red-green colorblind readers. Using ggplot2 we can access a whole range of colourblind friendly palettes: one package that has a whole range is RColorBrewer install it then try running RColorBrewer::display.brewer.all(colorblindFriendly = TRUE) what do you think you've asked your computer to show you? ## boxplot ggplot(penguins_nafree,aes(x = species, y = flipper_length_mm)) + geom_boxplot() + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;&quot;) + theme_classic() ## yet another theme ## violin plot ggplot(penguins_nafree,aes(x = species, y = flipper_length_mm)) + geom_violin() + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;&quot;) + theme_minimal() ## soon you could be making your own ## histogram, with a colorblind friendly palette ## try running display.brewer.all(colorblindFriendly = TRUE) ## what do you think it&#39;s doing ggplot(penguins_nafree,aes(x = flipper_length_mm)) + geom_histogram(aes(fill = species), alpha = 0.5, position = &quot;identity&quot;) + xlab(&quot;Flipper length (mm)&quot;) + scale_fill_brewer(palette = &quot;Dark2&quot;, name = &quot;Species&quot;) + theme_light() 2.4.4 What do we think about when we look at plots (a taster) Between group variation Within group variation 2.5 Other resources: optional but recommended Why data sovereignty matters Indigenous Data Sovereignty and Policy Principles of MƒÅori Data Sovereignty ggplot2 cheatsheet Elegant Graphics for Data Analysis Using ggplot2 to communicate your results Teacups, giraffes, and statistics: free online introductory level R and statistics modules "],["hypothesis-testing-and-introduction-to-linear-regression.html", "3 Hypothesis testing and introduction to linear regression 3.1 Learning objectives 3.2 Introdcution to hypothesis testing 3.3 üò± Correctly interpreting p-values üò± 3.4 Power, Significance, and multiple comparisons 3.5 Randomization test 3.6 One way ANOVA using lm() 3.7 Model diagnostics 3.8 Other resources: optional but recommended", " 3 Hypothesis testing and introduction to linear regression 3.1 Learning objectives Formulate a question/hypothesis to investigate based on the given data Explain and discuss the limitations of statistical linear regression, with a single factor explanatory variable Interpret and communicate the estimated coefficients of a linear regression model with a single factor explanatory variable to both a statistical and non-statistical audience; discuss and critique model fit List the aims and write out the appropriate null and alternative hypothesis using statistical notation for a one-sample t-test two-sample t-test (independent and dependent) randomization test one-way Analysis of Variance (ANOVA) Write R code to carry out a randomization test Use the lm() function to carry out a two-sample t-test (independent) one-way Analysis of Variance (ANOVA) Correctly interpret and communicate a p-value in terms of the hypotheses test listed above State in terms of probability statements the meaning of the power and significance level of an hypothesis test 3.2 Introdcution to hypothesis testing Using the paua.csv data from CANVAS. The P\\(\\overline{\\text{a}}\\)ua dataset contains the following variables Age of P\\(\\overline{\\text{a}}\\)ua in years (calculated from counting rings in the cone) Length of P\\(\\overline{\\text{a}}\\)ua shell in centimeters Species of P\\(\\overline{\\text{a}}\\)ua: Haliotis iris (typically found in NZ) and Haliotis australis (less commonly found in NZ) library(tidyverse) paua &lt;- read_csv(&quot;paua.csv&quot;) glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, ‚Ä¶ ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6‚Ä¶ ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, ‚Ä¶ 3.2.1 One-Sample t-test Using a violin plot we can look at the distribution of shell Length. We can calculate the average Length of all shells in our sample paua %&gt;% summarise(average_length = mean(Length)) ## # A tibble: 1 x 1 ## average_length ## &lt;dbl&gt; ## 1 5.19 What about drawing inference? Do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells is, say, 5cm? We know our sample average, but can we make any claims based on this one number? How do we reflect our uncertainty about the population mean? (remember it's the population we want to make inference on based on our sample!) Enter the Standard Error of the Mean, SEM, \\(= \\frac{\\sigma}{\\sqrt{n}}\\); where \\(\\sigma = \\sqrt{\\frac{\\Sigma_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) (\\(i = 1,...,n\\)) is the standard deviation (SD) of the sample, \\(n\\) is the sample size, and \\(\\bar{x}\\) is the sample mean. Calculating \\(\\Sigma_{i = 1}^n(x_i - \\bar{x})^2, i = 1,...,n\\) by hand. It's the sum squared differences of the distances between the \\(i^{th}\\) observation and the sample mean \\(\\bar{x}\\) (denoted \\(\\mu_x\\) in the GIF below) So using the example values in the GIF ## our sample of values x &lt;- c(1,2,3,5,6,9) ## sample mean sample_mean &lt;- mean(x) sample_mean ## [1] 4.333333 ## distance from mean for each value distance_from_mean &lt;- x - sample_mean distance_from_mean ## [1] -3.3333333 -2.3333333 -1.3333333 0.6666667 1.6666667 4.6666667 ## squared distance from mean for each value squared_distance_from_mean &lt;- distance_from_mean^2 squared_distance_from_mean ## [1] 11.1111111 5.4444444 1.7777778 0.4444444 2.7777778 21.7777778 ## sum of the squared distances sum(squared_distance_from_mean) ## [1] 43.33333 Calculating SD and SEM Now what about the SD? Remember it's the \\(\\sqrt{\\frac{\\Sigma_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) so = \\(\\sqrt{\\frac{43.3333333}{n-1}}\\) = \\(\\sqrt{\\frac{43.3333333}{6-1}}\\) = \\(\\sqrt{\\frac{43.3333333}{5}}\\) = 2.9439203. Or we could just use R's sd() function sd(x) ## [1] 2.94392 So the SEM is \\(\\frac{\\text{SD}}{\\sqrt{n}}\\) = \\(\\frac{2.9439203}{\\sqrt{6}}\\) In R sd(x)/sqrt(length(x)) ## [1] 1.20185 For the paua data we can simply use the in-built functions in R to calculate the SEM sem &lt;- paua %&gt;% summarise(mean = mean(Length), sem = sd(Length)/sqrt(length(Length))) sem ## # A tibble: 1 x 2 ## mean sem ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.19 0.155 Visualising the uncertainty Recall that the SEM is a measure of uncertainty about the mean. So we can use it to express our uncertainty visually. Typically \\(\\pm\\) twice the SEM is the interval used: Why error bars that are \\(\\pm\\) twice the SEM? This is approximately the 95% confidence interval for the population mean (see lecture) The exact 95% CI is given by \\(\\bar{x}\\) (mean) \\(\\pm\\) \\(t_{df,1 - \\alpha/2}\\) \\(\\times\\) SEM df = degrees of freedom (in this situation df = n - 1) \\(\\alpha\\) = level of significance Each mean has its own confidence interval whose width depends on the SEM for that mean When the df (more on these later) are large (e.g. 30 or greater) and \\(\\alpha\\) = 0.05 \\(t_{df,1 - \\alpha/2}\\) = \\(t_{large,0.975}\\) \\(\\approx\\) 2. Hence, the 95% confidence interval for the population mean is approximately \\(\\bar{x}\\) (mean) \\(\\pm\\) 2 \\(\\times\\) SEM Back to our hypothesis test Question: Do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells is 5cm? Formalizing into a hypothesis test: Null hypothesis: On average P\\(\\overline{\\text{a}}\\)ua shells are 5cm long Alternative hypothesis: On average P\\(\\overline{\\text{a}}\\)ua shells are not 5cm long Notationally: \\(H_0: \\mu = 5\\) vs \\(H_1: \\mu \\neq 5\\) (\\(\\mu\\) is the proposed mean) Calculating a statistic (we use a t-statistic) t-statistic $ = $ = \\(\\frac{5.1925 - 5}{0.155351}\\) = 1.239 \\(\\bar{x}\\) is the sample mean \\(\\mu\\) is the theoretical value (proposed mean) The corresponding p-value Q: What is a p-Value? A: Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data would be equal to or more extreme than its observed value So in this case it's the probability, under the null hypothesis (\\(\\mu = 5\\)), that we would observe a statistic as least as extreme as we did. Under our null hypothesis the distribution of the t-statistic is as below. The one calculated from our hypothesis test was 1.2391. Now, remember that our alternative hypotheses was \\(H_1: \\mu \\neq 5\\) so we have to consider both sides of the inequality; hence, anything as least as extreme is either \\(&gt; 1.2391\\) or \\(&lt; -1.2391\\) to our observed statistic (vertical lines). Anything as least as extreme is therefore given by the grey shaded areas. We can calculate the p-value using the pt() function (where q is our calculated t-statistic, and df are the degrees of freedom from above): 2*(1 - pt(q = 1.2391,df = 59)) ## [1] 0.2202152 Or we could do all of the above in one step using R t.test(paua$Length, mu = 5 ) ## ## One Sample t-test ## ## data: paua$Length ## t = 1.2391, df = 59, p-value = 0.2202 ## alternative hypothesis: true mean is not equal to 5 ## 95 percent confidence interval: ## 4.881643 5.503357 ## sample estimates: ## mean of x ## 5.1925 Recall, that the p-value gives the probability that under our null hypothesis we observe anything as least as extreme as what we did (hence the \\(\\times 2\\), think of the grey shaded area in the graph). This probability is \\(\\sim\\) 22%. Do you think what we've observed is likely under the null hypothesis? Does this plot help? The proposed mean is shown by the red horizontal line; the dashed line shows the sample mean and the dotted lines \\(\\pm\\) the SEM. 3.2.2 Differences between two means Calculating the differences between species means: Haliotis australis average - Haliotis iris average = \\(\\mu_{\\text{Haliotis australis}} - \\mu_{\\text{Haliotis iris}}\\) = 5.767 - 4.81 = 0.957.Doesn't really tell us much... As above the average values are all well and good, but what about variation? Recall the SEM from the one-sample t-test? The same idea holds here, although the calculation is a little bit more complicated (as we have to think about the number of observations in each group). But from the two group SEMs we can calculate the Standard Error of the Difference between two means, SED. 3.2.2.1 Independent samples t-test with lm() and t.test() Question: Do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells Formalizing into a hypothesis test: Null hypothesis: On average the species' shells are the same length Alternative hypothesis: they aren't! Notationally: \\(H_0: \\mu_{\\text{Haliotis iris}} - \\mu_{\\text{Haliotis australis}} = 0\\) vs \\(H_1: \\mu_{\\text{Haliotis iris}} \\neq \\mu_{\\text{Haliotis australis}}\\) \\(\\mu_{j}\\) is the average length for species $j = $ (Haliotis iris, Haliotis australis), Calculate the test statistic: t-statistic = \\(\\frac{\\bar{x}_{\\text{difference}} - \\mu}{\\text{SED}}\\) = \\(\\frac{\\bar{x}_{\\text{difference}} - 0}{\\text{SED}}\\) where \\(\\bar{x}_{\\text{difference}}\\) is the differences between the species` averages. Calculations area a little bit more tricky here so let's use R. We have two options (both answer our question): Option 1. using t.test() test &lt;- t.test(Length ~ Species, data = paua) ## printing out the result test ## ## Welch Two Sample t-test ## ## data: Length by Species ## t = 3.5404, df = 57.955, p-value = 0.0007957 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4158802 1.4980086 ## sample estimates: ## mean in group Haliotis australis mean in group Haliotis iris ## 5.766667 4.809722 test$p.value ## [1] 0.0007956853 Listed are the t-statistic, t = 3.5403636 and the p-value, p-value = 7.956853310^{-4} for the hypothesis test outlined above. What would you conclude? Option 2. using lm() t.lm &lt;- lm(Length ~ Species, data = paua) ##extracting the estimated parameters summary(t.lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.7666667 0.2278199 25.312396 4.911173e-33 ## SpeciesHaliotis iris -0.9569444 0.2941142 -3.253649 1.902460e-03 So, what are the printed values? Inference (Intercept) = the baseline = \\(\\mu_\\text{Haliotis australis}\\) = 5.7666667 SE of (Intercept) = SE of \\(\\mu_\\text{Haliotis australis}\\) = SEM = 0.2278199 \\(\\text{SpeciesHaliotis iris}\\) = \\(\\mu_\\text{Haliotis iris}\\) ‚Äì \\(\\mu_\\text{Haliotis australis}\\) = -0.9569444 SE of \\(\\text{SpeciesHaliotis iris}\\) = SE of (\\(\\mu_\\text{Haliotis iris}\\) ‚Äì \\(\\mu_\\text{Haliotis australis}\\) ) = SED = 0.2941142 Hypotheses being tested The t value and Pr (&gt;|t|) are the t - and p-value for testing the null hypotheses: Mean abundance is zero for Haliotis australis (not interested in this really) No difference between the population means of Haliotis australis and Haliotis iris ## changing the baseline ## it&#39;s the ordering that makes the difference paua_rl &lt;- paua %&gt;% mutate(Species = fct_relevel(Species, &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;)) c.lm &lt;- lm(Age ~ Species, data = paua_rl) summary(c.lm) ## ## Call: ## lm(formula = Age ~ Species, data = paua_rl) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9050 -0.9227 -0.0752 0.5973 5.5988 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.4029 0.2659 16.560 &lt; 2e-16 *** ## SpeciesHaliotis australis 3.1431 0.4204 7.477 4.63e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.595 on 58 degrees of freedom ## Multiple R-squared: 0.4908, Adjusted R-squared: 0.482 ## F-statistic: 55.9 on 1 and 58 DF, p-value: 4.631e-10 Inference (Intercept) = the baseline = \\(\\mu_\\text{Haliotis iris}\\) = 4.4028662 SE of (Intercept) = SE of \\(\\mu_\\text{Haliotis iris}\\) = SEM = 0.26587 \\(\\text{SpeciesHaliotis australis}\\) = \\(\\mu_\\text{Haliotis australis}\\) ‚Äì \\(\\mu_\\text{Haliotis iris}\\) = 3.1430733 SE of \\(\\text{SpeciesHaliotis australis}\\) = SE of (\\(\\mu_\\text{Haliotis australis}\\) ‚Äì \\(\\mu_\\text{Haliotis iris}\\) ) = SED = 0.4203774 Hypotheses being tested The t value and Pr (&gt;|t|) are the t - and p-value for testing the null hypotheses: Mean abundance is zero for Haliotis iris (not interested in this really) No difference between the population means of Haliotis iris and Haliotis australis 3.3 üò± Correctly interpreting p-values üò± &quot;Good statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning.&quot; --- ASA Statement on p-Values What is a p-Value? Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. Note p-values can indicate how incompatible the data are with a specified statistical model p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold roper inference requires full reporting and transparency p-value, or statistical significance, does not measure the size of an effect or the importance of a result by itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis 3.3.1 The ASA Statement on p-Values: Context, Process, and Purpose Q: Why do so many colleges and grad schools teach p-val=0.05? A: Because that's still what the scientific community and journal editors use. üò± BUT IT SHOULDN'T BE üò± Q: Why do so many people still use p-val=0.05? A: Because that's what they were taught in college or grad school. üò±BUT THEY SHOULDN'T BEüò± 3.4 Power, Significance, and multiple comparisons Recall, we have two competing hypotheses (claims) relating to the true vale of some population characteristic (e.g., the population mean, denoted \\(\\mu\\)): Some terminology Type I error (false positive): declare a difference (i.e., reject \\(H_0\\)) when there is no difference (i.e. \\(H_0\\) is true). Risk of the Type I error is determined by the level of significance (which we set!) (i.e., \\(\\alpha =\\text{ P(Type I error)} = \\text{P(false positive)}\\). Artwork by @allison_horst Type II error (false negative): difference not declared (i.e., \\(H_0\\) not rejected) when there is a difference (i.e., \\(H_0\\) is false). Let \\(\\beta =\\) P(do not reject \\(H_0\\) when \\(H_0\\) is false); so, \\(1-\\beta\\) = P(reject \\(H_0\\) when \\(H_0\\) is false) = P(a true positive), which is the statistical power of the test. Artwork by @allison_horst Significance level = probability of a Type I error = probability of finding an effect that is not there (false positive). Power: the probability that the test correctly rejects the null hypothesis when the alternative hypothesis is true. probability of finding an effect that is there = 1 - probability of a Type II error (false negative). Reducing the chance of a Type I error increases the chance of a Type II error. They are inversely related. Type II error rate is determined by a combination of the following. Effect size (size of difference, of biological significance) between the true population parameters Experimental error variance Sample size Choice of Type I error rate (\\(\\alpha\\)) Each time we carry out a hypothesis test the probability we get a false positive result (Type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). 3.5 Randomization test The basic approach to randomization tests is straightforward: Decide on a metric to measure the effect in question (e.g., differences between group means) Calculate that test statistic on the observed data. Note this metric can be anything you wish For chosen number of times (i.e., nreps below) Shuffle the data labels Calculate the test statistic for the reshuffled data and retain Calculate the proportion of times your reshuffled statistics equal or exceed the observed typically here we use the absolute values as we'd be carrying out a two-tailed test this is the probability of such an extreme result under the null Reject or retain the null on the basis of this probability. Randomization Test on Two Independent Samples Do average lengths differ between Species? means &lt;- paua %&gt;% group_by(Species) %&gt;% summarise(means = mean(Length)) ggplot(paua,aes(x = Species, y = Length)) + geom_violin() + geom_point(alpha = 0.4) + ylab(&quot;Length (cms)&quot;) + xlab(&quot;&quot;) + theme_classic() + geom_point(data = means, aes(x = Species, y = means, color = Species), size = 2) + geom_hline(data = means, aes(yintercept = means, color = Species), lty = 2, alpha = 0.5) + theme(legend.position = &quot;none&quot;) + geom_text(data = means, aes(x = Species, y = means + 0.3, label = paste0(&quot;Species averege = &quot;,round(means,3)), color = Species)) ggplot(paua,aes(x = Length, fill = Species)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.3) + xlab(&quot;Length (cms)&quot;) + ylab(&quot;&quot;) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. But because the data are skewed and we've likely got non-constant variances we may be better off adopting a randomization test, rather than a parametric t-test ## observed differences in means diff_in_means &lt;- (paua %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Length)) %&gt;% summarise(diff = diff(mean)))$diff diff_in_means ## [1] -0.9569444 ## Number of times I want to randomise nreps &lt;- 1000 ## initialize empty array to hold results randomisation_difference_mean &lt;- numeric(nreps) set.seed(1234) ## *****Remove this line for actual analyses***** ## This means that each run with produce the same results and ## agree with the printout that I show. for (i in 1:nreps) { ## the observations data &lt;- data.frame(value = paua$Length) ## randomise labels data$random_labels &lt;-sample(paua$Species, replace = FALSE) ## randomised differences in mean randomisation_difference_mean[i] &lt;- (data %&gt;% group_by(random_labels) %&gt;% summarise(mean = mean(value)) %&gt;% summarise(diff = diff(mean)))$diff } ## results results &lt;- data.frame(randomisation_difference_mean = randomisation_difference_mean) Interpreting p-values for a randomisation test ## How many randomised differences in means are as least as extreme as the one we observed ## absolute value as dealing with two tailed n_exceed &lt;- sum(abs(results$randomisation_difference_mean) &gt;= abs(diff_in_means)) n_exceed ## [1] 1 ## proportion n_exceed/nreps ## [1] 0.001 ggplot(results, aes(x = randomisation_difference_mean)) + geom_histogram() + theme_classic() + ylab(&quot;&quot;) + xlab(&quot;Differences between randomised group means&quot;) + geom_vline(xintercept = diff_in_means, col = &quot;cyan4&quot;, size = 1,alpha = 0.6) + annotate(geom = &#39;text&#39;, label = &quot;Observed difference between means&quot; , x = -Inf, y = Inf, hjust = 0, vjust = 1.5, color = &quot;cyan4&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. How would the parametric t-test have served? t.test(Length ~ Species, data = paua) ## ## Welch Two Sample t-test ## ## data: Length by Species ## t = 3.5404, df = 57.955, p-value = 0.0007957 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4158802 1.4980086 ## sample estimates: ## mean in group Haliotis australis mean in group Haliotis iris ## 5.766667 4.809722 Not too different after all Note In experimental situations a large p-value (large tail proportion) means that the luck of the randomisation quite often produces group differences as large or even larger than what we've got in our data. A small p-value means that the luck of the randomisation draw hardly ever produces group differences as large as we've got in our data. Statistical significance does not imply practical significance. Statistical significance says nothing about the size of treatment differences. To estimate the sizes of differences you need confidence intervals. NOTE: We can extend the randomization test to make inference about any sample statistic (not just the mean) 3.6 One way ANOVA using lm() Remember the penguins? You might find this application useful, now and later... Now we have more than two groups: \\(3\\) potential comparisons we might be interested in. Remember that each time we carry out a hypothesis test the probability we get a false positive result (Type I error) is given by \\(\\alpha\\) (the level of significance we choose). In light of this we should control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). ANalysis Of VAriance (ANOVA): this can, again, be done using lm() fit.lm &lt;- lm(bill_depth_mm ~ species, data = penguins_nafree) Taking species Adelie as the baseline.. summary(fit.lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.34726027 0.09299608 197.2906740 0.000000e+00 ## speciesChinstrap 0.07332796 0.16497460 0.4444803 6.569867e-01 ## speciesGentoo -3.35062162 0.13877592 -24.1441135 6.622121e-75 (Intercept) = \\(\\text{mean}_{\\text{Adelie}}\\) = 18.3472603 SE of (Intercept) = SE of \\(\\text{mean}_{\\text{Adelie}}\\) = SEM = 18.3472603 \\(\\text{speciesChinstrap}\\) = \\(\\text{mean}_{\\text{Chinstrap}}\\) - \\(\\text{mean}_{\\text{Adelie}}\\) = 0.073328 SE of \\(\\text{speciesChinstrap}\\) = SE of (\\(\\text{mean}_{\\text{Chinstrap}}\\) - \\(\\text{mean}_{\\text{Adelie}}\\) ) = SED = 0.1649746 What is \\(\\text{mean}_{\\text{Gentoo}}\\) - \\(\\text{mean}_{\\text{Adelie}}\\)? Hypotheses being tested The t value and Pr (&gt;|t|) are the t - and p-value for testing the null hypotheses Mean abundance is zero for Adelie population No difference between the population means of Chinstrap and Adelie No difference between the population means of Gentoo and Adelie We're interested in 2 and 3, but not necessarily 1! What do you conclude? Does your inference match the plot? 3.7 Model diagnostics Carrying out any linear regression we have some key assumptions Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed Diagnostic plots gglm::gglm(fit.lm) # Plot the four main diagnostic plots Residuals vs Fitted plot You are basically looking for no pattern or structure in your residuals (e.g., a &quot;starry&quot; night). You definitely don't want to see is the scatter increasing around the zero line (dashed line) as the fitted values get bigger (e.g., think of a trumpet, a wedge of cheese, or even a slice of pizza) which would indicate unequal variances (heteroscedacity). [Note: things look a bit weird, points are in clumps? Why? Well think of the model we fitted (i.e., one with a single factor explanatory variable)] Normal quantile-quantile (QQ) plot This plot shows the sorted residuals versus expected order statistics from a standard normal distribution. Samples should be close to a line; points moving away from 45 degree line at the tails suggest the data are from a skewed distribution. Scale-Location plot (\\(\\sqrt{\\text{|standardized residuals vs Fitted|}}\\)) Another way to check the homoskedasticity (constant-variance) assumption. We want the line to be roughly horizontal. If this is the case then the average magnitude of the standardized residuals isn't changing much as a function of the fitted values. We'd also like the spread around the line not to vary much with the fitted values; then the variability of magnitudes doesn't vary much as a function of the fitted values. Residuals vs Leverage plot (standardized residuals vs Leverage) This can help detect outliers in a linear regression model. For linear regression model leverage measures how sensitive a fitted value is to a change in the true response. We're looking at how the spread of standardized residuals changes as the leverage. This can also be used to detect heteroskedasticity and non-linearity: the spread of standardized residuals shouldn't change as a function of leverage. In addition, points with high leverage may be influential: that is, deleting them would change the model a lot. [Note: things look a bit weird: there are only three points? Why? Well think of the model we fitted (i.e., one with a single factor explanatory variable). We can't tell much here, bu twill use these plots more later on.] 3.8 Other resources: optional but recommended The ASA Statement on p-Values: Context, Process, and Purpose I thought it could be helpful to have a thread on ANOVA in R. As a statistical consultant, this is the most frequent FAQ I get from clients - how to run a linear model on their data, conduct hypothesis tests, extract predicted means and perform contrasts. ‚Äî We are R-Ladies (@WeAreRLadies) February 2, 2020 I've made this cheat sheet and I think it's important. Most stats 101 tests are simple linear models - including &quot;non-parametric&quot; tests. It's so simple we should only teach regression. Avoid confusing students with a zoo of named tests. https://t.co/9PFR1ly3lW 1/n ‚Äî Jonas K. Lindel√∏v (@jonaslindeloev) March 27, 2019 The aov() function in #Rstats is actually a wrapper around the lm() function pic.twitter.com/FbvxQdtD4c ‚Äî Dan Quintana (@dsquintana) October 30, 2019 "],["statistical-inference.html", "4 Statistical Inference 4.1 Learning Objectives 4.2 Regression 4.3 Model, comparison, selection, and checking (again) 4.4 Confidence intervals 4.5 TL;DR lm() 4.6 Other resources: optional but recommended 4.7 Beyond Linear Models to Generalised Linear Models (GLMs) (not examinable)", " 4 Statistical Inference 4.1 Learning Objectives Carry out and interpret tests for the existence of relationships between explanatory variables and the response in a linear model Write R code to fit a linear model with a single continuous explanatory variable Write R code to fit a linear model with a continuous explanatory variable and a factor explanatory variable Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of the intercept the effect of a factor the effect of a one-unit increase in a numeric variable the effect of an x-unit increase in a numeric variable Make a point prediction of the response for a new observation Write R code to fit a linear model with interaction terms in the explanatory variables Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of main effects in a model with an interaction the effect of one variable when others are included in the model Explain why you may want to include interaction effects in a linear model Describe the differences between the operators : and * in an R model-fitting formula 4.2 Regression 4.2.1 Some mathematical notation Let's consider a linear regression with a simple explanatory variable: \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2).\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response \\(x_i\\) is the value of the explanatory variable \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\) is the intercept term (a parameter to be estimated), and \\(\\beta_1\\) is the slope: coefficient of the explanatory variable (a parameter to be estimated), and Does this remind you of anything? 4.2.2 Modeling Bill Depth Key assumptions Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed library(tidyverse) library(palmerpenguins) penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(data = penguins_nafree, aes(x = bill_depth_mm)) + geom_histogram() + theme_classic() + xlab(&quot;Bill depth (mm)&quot;) First off let's fit a null (intercept only) model. This in old money would be called a one sample t-test. slm_null &lt;- lm(bill_depth_mm ~ 1, data = penguins_nafree) summary(slm_null)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.16486 0.1079134 159.0614 1.965076e-315 Model formula The (Intercept) term, 17.1648649, tells us the average value of the response (bill_depth_mm), see penguins_nafree %&gt;% summarise(average_bill_depth = mean(bill_depth_mm)) ## # A tibble: 1 x 1 ## average_bill_depth ## &lt;dbl&gt; ## 1 17.2 Inference The SEM (Std. Error) = 0.1079134. The hypothesis being tested is \\(H_0:\\) ((Intercept) ) \\(\\text{mean}_{\\text{`average_bill_depth`}} = 0\\) vs. \\(H_1:\\) ((Intercept)) \\(\\text{mean}_{\\text{`average_bill_depth`}} \\neq 0\\) The t-statistic is given by t value = Estimate + Std. Error = 159.0614207 The p-value is given byPr (&gt;|t|) = 1.965076110^{-315}. So the probability of observing a t-statistic as least as extreme given under the null hypothesis (average bill depth = 0) given our data is 1.965076110^{-315}, pretty strong evidence against the null hypothesis I'd say! 4.2.3 Single continuous variable Does bill_length_mm help explain some of the variation in bill_depth_mm? p1 &lt;- ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p1 slm &lt;- lm(bill_depth_mm ~ bill_length_mm, data = penguins_nafree) Model formula \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\epsilon \\end{aligned} \\] Fitted model summary(slm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.78664867 0.85417308 24.335406 1.026904e-75 ## bill_length_mm -0.08232675 0.01926835 -4.272642 2.528290e-05 Here, the (Intercept): Estimate gives us the estimated average bill depth (mm) given the estimated relationship bill length (mm) and bill length. The bill_length_mm : Estimate (think of \\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.082mm decrease (or a -0.082mm increase) in bill depth. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm) ## plot ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) 4.2.4 Factor and a continous variable Adding species p2 &lt;- ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p2 slm_sp &lt;- lm(bill_depth_mm ~ bill_length_mm + species, data = penguins_nafree) Model formula \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] Fitted model summary(slm_sp)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.5652616 0.69092642 15.291442 2.977289e-40 ## bill_length_mm 0.2004431 0.01767974 11.337449 2.258955e-25 ## speciesChinstrap -1.9330779 0.22571878 -8.564099 4.259893e-16 ## speciesGentoo -5.1033153 0.19439523 -26.252267 1.043789e-82 Simpson's paradox... look how the slopes have switched direction from the model above. Remember when we have factor explanatory variables (e.g., species) we have to use dummy variables, see lecture. Here the Adelie group are the baseline (R does this alphabetically, to change this see previous chapter). Here, the (Intercept): Estimate gives us the estimated average bill depth (mm) of the Adelie penguins given the ther variables in the model. The bill_length_mm : Estimate (think of \\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.082mm decrease (or a -0.082mm increase) in bill depth. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm_sp) ## plot ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) 4.2.5 Interactions Recap \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2)\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response \\(z_i\\) is one explanatory variable \\(x_i\\) is another explanatory variable \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value But what about interactions? For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\beta_3z_ix_i + \\epsilon_i\\] \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2)\\] Note: to include interaction effects in our model by using either the * or : syntax in our model formula. See [Model formula syntax] for further details. slm_int &lt;- lm(bill_depth_mm ~ bill_length_mm*species, data = penguins_nafree) Model formula \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\beta_{4}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{5}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Gentoo}}) + \\epsilon \\end{aligned} \\] Fitted model summary(slm_int)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.48770713 1.15987305 9.9042797 2.135979e-20 ## bill_length_mm 0.17668344 0.02980564 5.9278518 7.793199e-09 ## speciesChinstrap -3.91856701 2.06730876 -1.8954919 5.890889e-02 ## speciesGentoo -6.36675118 1.77989710 -3.5770333 4.000274e-04 ## bill_length_mm:speciesChinstrap 0.04552828 0.04594283 0.9909769 3.224296e-01 ## bill_length_mm:speciesGentoo 0.03092816 0.04111608 0.7522157 4.524625e-01 ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm_int) ## plot ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) Are the non-parallel lines non-parallel enough to reject the parallel line model? (more below) 4.2.5.1 Model formula syntax In R to specify the model you want to fit you typically create a model formula object; this is usually then passed as the first argument to the model fitting function (e.g., lm()). Some notes on syntax: Consider the model formula example y ~ x + z + x:z. There is a lot going on here: The variable to the left of ~ specifies the response, everything to the right specify the explanatory variables + indicated to include the variable to the left of it and to the right of it (it does not mean they should be summed) : denotes the interaction of the variables to its left and right Additional, some other symbols have special meanings in model formula: * means to include all main effects and interactions, so a*b is the same as a + b + a:b ^ is used to include main effects and interactions up to a specified level. For example, (a + b + c)^2 is equivalent to a + b + c + a:b + a:c + b:c (note (a + b + c)^3 would also add a:b:c) - excludes terms that might otherwise be included. For example, -1 excludes the intercept otherwise included by default, and a*b - b would produce a + a:b Mathematical functions can also be directly used in the model formula to transform a variable directly (e.g., y ~ exp(x) + log(z) + x:z). One thing that may seem counter intuitive is in creating polynomial expressions (e.g., \\(x^2\\)). Here the expression y ~ x^2 does not relate to squaring the explanatory variable \\(x\\) (this is to do with the syntax ^ you see above. To include \\(x^2\\) as a term in our model we have to use the I() (the &quot;as-is&quot; operator). For example, y ~ I(x^2)). 4.3 Model, comparison, selection, and checking (again) Remember that it is always is imperative that we check the underlying assumptions of our model! If our assumptions are not met then basically the maths falls over and we can't reliably draw inference from the model (e.g., can't trust the parameter estimates etc.). Two of the most important assumption are: equal variances (homogeneity of variance), and normality of residuals. 4.3.1 Model comparison and selection Are the non-parallel lines non-parallel enough to reject the parallel line model? We can compare nested\\(^*\\) linear models by hypothesis testing. Luckily the R function anova() automates this. Yes the same idea as we've previously learnt about ANOVA! We essentially perform an F-ratio test between the nested models! \\(^*\\)By nested we mean that one model is a subset of the other (i.e., where some coefficients have been fixed at zero). For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\epsilon_i\\] is a nested version of \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] where \\(\\beta_2\\) has been fixed to zero. As an example consider testing the null model slm_null against the full model with interactions slm_int. To carry out the appropriate hypothesis test in R we can run anova(mod.no.c,mod.interaction) The Akaike information criterion (AIC) from Module 2 of the course. AICs an estimator of out-of-sample prediction error and can be used as a metric to choose between competing models. R already has an AIC() function that can be used directly on your lm() model object(s). For example, AIC(mod.add,mod.interaction) This backs up what our AIC values indicated. The p-value tells us that we have really strong evidence (p-value is exceptionally low) against the null model (mod.no.c) in favor of the model that includes interactions! As always it's important to do a sanity check! Does this make sense? Based on what we know about C02 emissions and food types I'd say YES! 4.4 Confidence intervals confint(multiple.mod) 4.4.1 Interpreting confidence intervals For every 1mm increase in mesosoma length we estimate the expected sting length to increases between round(confint(multiple.mod)[9,1],1) and round(confint(multiple.mod)[9,2],1) mm We estimate that the expected sting length of a species predated by Immature Hymenoptera is between round(confint(multiple.mod)[6,1],1) and round(confint(multiple.mod)[6,2],1) mm longer than that predated by Coleoptera with the same sting length 4.4.1.1 Point prediction For species predated by Immature Hymenoptera with mesosomal length 5mm \\[\\hat{\\text{Age}} = \\hat{\\beta_0} + \\hat{\\beta_5}*1 + \\hat{\\beta_8}*5\\] \\[\\downarrow\\] \\[\\hat{\\text{Age}} = -0.99 + 4.25*1 + 0.87*5\\] \\[\\downarrow\\] \\[7.61\\] 4.4.1.2 In R ## create new data frame with data we want to predict to newdata &lt;- data.frame(prey_graph = &quot;Immature Hymenoptera&quot;,mesosoma = 5) ## use predict() function predict(multiple.mod, newdata = newdata) ## round to 2 d.p round(predict(multiple.mod, newdata = newdata),2) 4.5 TL;DR lm() Traditional name Model formula R code Simple regression \\(Y \\sim X_{continuous}\\) lm(Y ~ X) One-way ANOVA \\(Y \\sim X_{categorical}\\) lm(Y ~ X) Two-way ANOVA \\(Y \\sim X1_{categorical} + X2_{categorical}\\) lm(Y ~ X1 + X2) ANCOVA \\(Y \\sim X1_{continuous} + X2_{categorical}\\) lm(Y ~ X1 + X2) Multiple regression \\(Y \\sim X1_{continuous} + X2_{continuous}\\) lm(Y ~ X1 + X2) Factorial ANOVA \\(Y \\sim X1_{categorical} * X2_{categorical}\\) lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2) Artwork by @allison_horst Meet your MLR teaching assistants Interpret coefficients for categorical predictor variables Interpret coefficients for continuous predictor variables Make predictions using the regression model Residuals Check residuals for normality 4.6 Other resources: optional but recommended Exploring interactions with continuous predictors in regression models The ASA Statement on p-Values: Context, Process, and Purpose 4.7 Beyond Linear Models to Generalised Linear Models (GLMs) (not examinable) Recall the assumptions of a linear model The \\(i\\)th observation's response, \\(Y_i\\), comes from a normal distribution Its mean, \\(\\mu_i\\), is a linear combination of the explanatory terms Its variance, \\(\\sigma^2\\), is the same for all observations Each observation's response is independent of all others But, what if we want to rid ourselves from a model with normal errors? The answer: Generalised Linear Models. 4.7.1 Counting animals... A normal distribution does not adequately describe the response, the number of animals It is a continuous distribution, but the response is discrete It is symmetric, but the response is unlikely to be so It is unbounded, and assumes it is plausible for the response to be negative I addition, a linear regression model typically assumes constant variance, but int his situation this unlikely to be the case. So why assume a normal distribution? Let's use a Poisson distribution instead. \\[\\begin{equation*} \\mu_i = \\beta_0 + \\beta_1 x_i, \\end{equation*}\\] So \\[\\begin{equation*} Y_i \\sim \\text{Normal}(\\mu_i\\, \\sigma^2), \\end{equation*}\\] becomes \\[\\begin{equation*} Y_i \\sim \\text{Poisson}(\\mu_i), \\end{equation*}\\] The Poisson distribution is commonly used as a general-purpose distribution for counts. A key feature of this distribution is \\(\\text{Var}(Y_i) = \\mu_i\\), so we expect the variance to increase with the mean. 4.7.2 Other modelling approaches (for interest only) R function Use glm() Fit a linear model with a specific error structure specified using the family = argument (Poisson, binomial, gamma) gam() Fit a generalised additive model. The R package mgcv must be loaded lme() and nlme() Fit linear and non-linear mixed effects models. The R package nlme must be loaded lmer() Fit linear and generalised linear and non-linear mixed effects models. The package lme4 must be installed and loaded gls() Fit generalised least squares models. The R package nlme must be loaded "],["experimental-design.html", "5 Experimental Design", " 5 Experimental Design "],["multivariate-data-methods.html", "6 Multivariate Data Methods", " 6 Multivariate Data Methods "]]
