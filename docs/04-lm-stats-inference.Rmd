# Statistical Inference

## Learning Objectives

+ Carry out and interpret tests for the existence of relationships between explanatory variables and the response in a linear model
   + Write R code to fit a linear model with a single continuous explanatory variable
   + Write R code to fit a linear model with a continuous explanatory variable and a factor explanatory variable
   + Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of
      + the intercept
      + the effect of a factor
      + the effect of a one-unit increase in a numeric variable
      + the effect of an x-unit increase in a numeric variable
   + Make a point prediction of the response for a new observation
+ Write R code to fit a linear model with interaction terms in the explanatory variables
  + Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of
     + main effects in a model with an interaction
     + the effect of one variable when others are included in the model
  + Explain why you may want to include interaction effects in a linear model
  + Describe the differences between the operators `:` and `*` in an `R` model-fitting formula
  


## Regression

![](https://raw.githubusercontent.com/cmjt/statbiscuits/master/figs_n_gifs/lm.gif)

### Some mathematical notation

Let's consider a linear regression with a simple explanatory variable:

$$Y_i = \alpha + \beta_1x_i + \epsilon_i$$
where

$$\epsilon_i \sim \text{Normal}(0,\sigma^2).$$

Here for observation $i$

  + $Y_i$ is the value of the response 
  + $x_i$ is the value of the explanatory variable 
  + $\epsilon_i$ is the error term: the difference between $Y_i$ and its expected value
  + $\alpha$ is the intercept term (a parameter to be estimated), and 
  + $\beta_1$ is the slope: coefficient of the explanatory variable (a parameter to be estimated), and 

Does this remind you of anything?

![](https://memegenerator.net/img/instances/63099571/one-does-not-simply-forget-ymxc.jpg)


### Modeling Bill Depth 

![](https://cran.r-project.org/web/packages/palmerpenguins/readme/man/figures/culmen_depth.png)

**Key assumptions**

+ **Independence** 
+ There is a **linear relationship** between the response and the explanatory variables
+ The residuals have **constant variance**
+ The **residuals** are normally distributed

```{r, message = FALSE, warning=FALSE}
library(tidyverse)
library(palmerpenguins)
penguins_nafree <- penguins %>% drop_na()
```

```{r, message=FALSE}
ggplot(data = penguins_nafree, aes(x = bill_depth_mm)) +
  geom_histogram() + theme_classic() +
  xlab("Bill depth (mm)")

```


First off let's fit a null (intercept only)  model. This in *old money* would be called a one sample t-test. 

```{r null}
slm_null <- lm(bill_depth_mm ~ 1, data = penguins_nafree)
summary(slm_null)$coef
```

**Model formula**

 The `(Intercept)` term, `r summary(slm_null)$coef[1,1]`, tells us the average value of the response (`bill_depth_mm`), see

```{r av}
penguins_nafree %>% summarise(average_bill_depth = mean(bill_depth_mm))
```

**Inference**

The SEM (`Std. Error`) = `r summary(slm_null)$coef[1,2]`. 

The hypothesis being tested is $H_0:$ (`(Intercept)` ) $\text{mean}_{\text{`average_bill_depth`}} = 0$ vs. $H_1:$ (`(Intercept)`) $\text{mean}_{\text{`average_bill_depth`}} \neq 0$ 

The t-statistic is given by `t value` = `Estimate` + `Std. Error` =  `r summary(slm_null)$coef[1,3]`

The p-value is given by`Pr (>|t|)` =  `r summary(slm_null)$coef[1,4]`.

So the probability of observing a t-statistic as least as extreme given under the null hypothesis (average bill depth = 0) given our data is `r summary(slm_null)$coef[1,4]`, pretty strong evidence against the null hypothesis I'd say!
	

### Single continuous variable

**Does `bill_length_mm` help explain some of the variation in `bill_depth_mm`?**

```{r}
p1 <- ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point() + ylab("Bill depth (mm)") +
  xlab("Bill length (mm)") + theme_classic()
p1
```

```{r slm}
slm <- lm(bill_depth_mm ~ bill_length_mm, data = penguins_nafree)
```



**Model formula**

```{r, echo = FALSE, results='asis'}
library(equatiomatic)
extract_eq(slm, wrap = TRUE)
```

**Fitted model**

```{r}
summary(slm)$coef
```

Here, the `(Intercept)`: `Estimate` gives us the estimated average bill depth (mm) **given the estimated relationship** bill length (mm) and bill length.

The `bill_length_mm` : `Estimate` (think of $\beta_1$ above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a `r -1*round(summary(slm)$coef[2,1],3)`mm decrease (or a `r round(summary(slm)$coef[2,1],3)`mm increase) in bill depth.

```{r}
## calculate predicted values
penguins_nafree$pred_vals <- predict(slm)
## plot
ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point() + ylab("Bill depth (mm)") +
  xlab("Bill length (mm)") + theme_classic() +
  geom_line(aes(y = pred_vals))

```

### Factor and a continous variable

**Adding** `species`

```{r}
p2 <- ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) +
  geom_point() + ylab("Bill depth (mm)") +
  xlab("Bill length (mm)") + theme_classic() 
p2
```

```{r}
slm_sp <- lm(bill_depth_mm ~ bill_length_mm + species, data = penguins_nafree)
```


**Model formula**

```{r, echo = FALSE, results='asis'}
extract_eq(slm_sp, wrap = TRUE)
```

**Fitted model**


```{r}
summary(slm_sp)$coef
```

*Simpson's paradox...* look how the slopes have switched direction from the model above.

Remember when we have factor explanatory variables (e.g., `species`) we have to use dummy variables, see lecture. Here the **Adelie** group are the baseline (`R` does this alphabetically, to change this see previous chapter).


Here, the `(Intercept)`: `Estimate` gives us the estimated average bill depth (mm) of the *Adelie* penguins **given the ther variables in the model**.

The `bill_length_mm` : `Estimate` (think of $\beta_1$ above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a `r -1*round(summary(slm)$coef[2,1],3)`mm decrease (or a `r round(summary(slm)$coef[2,1],3)`mm increase) in bill depth.

```{r}
## calculate predicted values
penguins_nafree$pred_vals <- predict(slm_sp)
## plot
ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) +
  geom_point() + ylab("Bill depth (mm)") +
  xlab("Bill length (mm)") + theme_classic()  +
  geom_line(aes(y = pred_vals))

```



### Interactions

Recap

$$Y_i = \beta_0 + \beta_1z_i + \beta_2x_i + \epsilon_i$$
$$\epsilon_i \sim \text{Normal}(0,\sigma^2)$$
Here for observation $i$

  + $Y_i$ is the value of the response
  + $z_i$ is one explanatory variable
  + $x_i$ is another explanatory variable
  + $\epsilon_i$ is the error term: the difference between $Y_i$ and its expected value
  
**But**  what about interactions? For example, 

$$Y_i = \beta_0 + \beta_1z_i + \beta_2x_i + \beta_3z_ix_i + \epsilon_i$$
$$\epsilon_i \sim \text{Normal}(0,\sigma^2)$$

**Note:** to include interaction effects in our model by using either the `*` or `:` syntax in our model formula. See [Model formula syntax] for further details.

```{r}
slm_int <- lm(bill_depth_mm ~ bill_length_mm*species, data = penguins_nafree)

```


**Model formula**

```{r, echo = FALSE, results='asis'}
extract_eq(slm_int, wrap = TRUE)
```

**Fitted model**

```{r}
summary(slm_int)$coef
```

```{r}
## calculate predicted values
penguins_nafree$pred_vals <- predict(slm_int)
## plot
ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) +
  geom_point() + ylab("Bill depth (mm)") +
  xlab("Bill length (mm)") + theme_classic()  +
  geom_line(aes(y = pred_vals))

```


**Are the non-parallel lines non-parallel enough to reject the parallel line model?** (*more below*)

#### **Model formula** syntax

In `R` to specify the model you want to fit you typically create a model formula object; this is usually then passed as the first argument to the model fitting function (e.g., `lm()`).

Some notes on syntax:

Consider the model formula example `y ~ x + z + x:z`. There is a lot going on here:

 + The variable to the left of `~` specifies the response, everything to the right specify the explanatory variables
 + `+` indicated to include the variable to the left of it and to the right of it (it does **not** mean they should be summed)
 + `:` denotes the interaction of the variables to its left and right
 
Additional, some other symbols have special meanings in model formula:

 + `*` means to include all main effects and interactions, so `a*b` is the same as `a + b + a:b`
 
 + `^` is used to include main effects and interactions up to a specified level. For example, `(a + b + c)^2` is equivalent to `a + b + c + a:b + a:c + b:c` (note `(a + b + c)^3` would also add `a:b:c`)
 + `-` excludes terms that might otherwise be included. For example, `-1` excludes the intercept otherwise included by default, and `a*b - b` would produce `a + a:b`
 
Mathematical functions can also be directly used in the model formula to transform a variable directly (e.g., `y ~ exp(x) + log(z) + x:z`). One thing that may seem counter intuitive is in creating polynomial expressions (e.g., $x^2$). Here the expression `y ~ x^2` does **not** relate to squaring the explanatory variable $x$ (this is to do with the syntax `^` you see above. To include $x^2$ as a term in our model we have to use the `I()` (the "as-is" operator). For example, `y ~ I(x^2) `).


## Model, comparison, selection, and checking (again)


Remember that it is always is imperative that we **check the underlying assumptions** of our model! If our assumptions are not met then basically the maths falls over and we can't reliably draw inference from the model (e.g., can't trust the parameter estimates etc.). Two of the most important assumption are:

  + equal variances (homogeneity of variance), and 
  
  + normality of residuals. 
  

### Model comparison and selection

**Are the non-parallel lines non-parallel enough to reject the parallel line model?** 

We can compare nested$^*$ linear models by hypothesis testing. Luckily the `R` function `anova()` automates this. Yes the same idea as we've previously learnt about ANOVA! We essentially perform an F-ratio test between the nested models! 

$^*$By nested we mean that one model is a subset of the other (i.e., where some coefficients have been fixed at zero). For example,

$$Y_i = \beta_0 + \beta_1z_i + \epsilon_i$$

is a nested version of

$$Y_i = \beta_0 + \beta_1z_i + \beta_2x_i + \epsilon_i$$ where $\beta_2$ has been fixed to zero.

As an example consider testing the null model `slm_null` against the full model with interactions `slm_int`. To carry out the appropriate hypothesis test in `R` we can run

```{r, echo = TRUE, eval = FALSE}
anova(mod.no.c,mod.interaction)

```

The Akaike information criterion (AIC) from Module 2 of the course. AICs an estimator of out-of-sample prediction error and can be used as a metric to choose between competing models. `R` already has an `AIC()` function that can be used directly on your `lm()` model object(s). For example,

```{r, echo = TRUE, eval = FALSE}
AIC(mod.add,mod.interaction)
```



This backs up what our AIC values indicated. The p-value tells us that we have really strong evidence (p-value is exceptionally low) against the null model (`mod.no.c`) in favor of the model that includes interactions!

As always it's important to do a sanity check! Does this make sense? Based on what we know about C02 emissions and food types I'd say YES!



## Confidence intervals

```{r, echo = TRUE, eval = FALSE}
confint(multiple.mod)
```

### Interpreting confidence intervals

+ For every 1mm increase in mesosoma length we estimate the expected sting length to increases between `round(confint(multiple.mod)[9,1],1)` and ` round(confint(multiple.mod)[9,2],1)` mm

+ We estimate that the expected sting length of a species predated by Immature Hymenoptera is between
`round(confint(multiple.mod)[6,1],1)` and `round(confint(multiple.mod)[6,2],1)` mm longer than that predated by Coleoptera with the same sting length

#### Point prediction

For species predated by Immature Hymenoptera with mesosomal length 5mm


$$\hat{\text{Age}} = \hat{\beta_0} + \hat{\beta_5}*1 + \hat{\beta_8}*5$$
$$\downarrow$$

$$\hat{\text{Age}} = -0.99 + 4.25*1 + 0.87*5$$
$$\downarrow$$

$$7.61$$


#### In `R`

```{r,echo = TRUE, eval = FALSE}
## create new data frame with data we want to predict to
newdata <- data.frame(prey_graph = "Immature Hymenoptera",mesosoma = 5)
## use predict() function
predict(multiple.mod, newdata = newdata)
## round to 2 d.p
round(predict(multiple.mod, newdata = newdata),2)
```



## TL;DR `lm()`


| Traditional name    | Model formula  | R code  |
| ------------------- |:--------------:| -------:|
| Simple regression   | $Y \sim X_{continuous}$ | `lm(Y ~ X)` |
| One-way ANOVA       | $Y \sim X_{categorical}$      |   `lm(Y ~ X)` |
| Two-way ANOVA       | $Y \sim X1_{categorical} + X2_{categorical}$| `lm(Y ~ X1 + X2)` |
| ANCOVA              | $Y \sim X1_{continuous} + X2_{categorical}$ |`lm(Y ~ X1 + X2)` |
| Multiple regression | $Y \sim X1_{continuous} + X2_{continuous}$ | `lm(Y ~ X1 + X2)` |
| Factorial ANOVA     | $Y \sim X1_{categorical} * X2_{categorical}$|   `lm(Y ~ X1 * X2)` or `lm(Y ~ X1 + X2 + X1:X2)` |

**[Artwork by \@allison_horst](https://github.com/allisonhorst/stats-illustrations)**


![Meet your MLR teaching assistants](https://github.com/allisonhorst/stats-illustrations/blob/master/other-stats-artwork/dragons.png?raw=true)

![Interpret coefficients for categorical predictor variables](https://github.com/allisonhorst/stats-illustrations/blob/master/other-stats-artwork/dragon_regression.png?raw=true)

![Interpret coefficients for continuous predictor variables](https://github.com/allisonhorst/stats-illustrations/blob/master/other-stats-artwork/dragons_continuous.png?raw=true)

![Make predictions using the regression model](https://github.com/allisonhorst/stats-illustrations/blob/master/other-stats-artwork/dragon_predict_mlr.png?raw=true)

![Residuals](https://github.com/allisonhorst/stats-illustrations/blob/master/other-stats-artwork/dragon_residual.png?raw=true)

![Check residuals for normality](https://github.com/allisonhorst/stats-illustrations/blob/master/other-stats-artwork/dragon_residual_distribution.png?raw=true)


## Other resources: optional but recommended

+ [Exploring interactions with continuous predictors in regression models](https://interactions.jacob-long.com/articles/interactions.html)
+ [The ASA Statement on p-Values: Context, Process, and Purpose](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)


## Beyond Linear Models to Generalised Linear Models (GLMs) (*not examinable*)

Recall the assumptions of a linear model

+ The $i$th observation's response, $Y_i$, comes from a normal distribution
+ Its mean, $\mu_i$, is a linear combination of the explanatory terms
+ Its variance, $\sigma^2$, is the same for all observations
+ Each observation's response is independent of all others
  
But, what if we want to rid ourselves from a model with normal errors? 

The answer: Generalised Linear Models.

### Counting animals... 

A normal distribution does not adequately describe the response, the number of animals

 + It is a continuous distribution, but the response is discrete
 + It is symmetric, but the response is unlikely to be so
 + It is unbounded, and assumes it is plausible for the response to be negative


I addition, a linear regression model typically assumes constant variance, but int his situation this unlikely to be the case.

So why assume a normal distribution? Let's use a Poisson distribution instead.

\begin{equation*}    
    \mu_i = \beta_0 + \beta_1 x_i,
  \end{equation*}

So 
  \begin{equation*}
    Y_i \sim \text{Normal}(\mu_i\, \sigma^2),
  \end{equation*}
  
becomes
  
\begin{equation*}
    Y_i \sim \text{Poisson}(\mu_i),
\end{equation*}
  
The Poisson distribution is commonly used as a general-purpose distribution for counts. A key feature of this distribution is $\text{Var}(Y_i) = \mu_i$, so we expect the variance to increase with the mean.

### Other modelling approaches (for interest only)

| `R` function    | Use                    | 
| --------------- |------------------------|
| `glm()`         | Fit a  linear model with a specific error structure specified using the `family =` argument (Poisson, binomial, gamma)|
| `gam()`         | Fit a generalised additive model. The R package `mgcv` must be loaded |
|`lme()` and `nlme()`| Fit linear and non-linear mixed effects models. The R package `nlme` must be loaded |
| `lmer()`        | Fit linear and generalised linear and non-linear mixed effects models. The package `lme4` must be installed and loaded |
| `gls()`         | Fit generalised least squares models. The R package `nlme` must be loaded |


