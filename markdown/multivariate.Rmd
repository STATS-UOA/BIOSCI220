---
title: "Visualising and analysing multivariate data"
author: "BIOSCI220"
date: "Semester 1 2020"
output:
  html_document:
    toc: true
    toc_float: true
    theme: yeti
    toc_depth: 3
    highlight: espresso
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE,message = FALSE, warning = FALSE,fig.align = "center")
```

## Learning Objectives

By the end of this lab student should be able to

   + Discuss the aims and motivations of Multidimensional Scaling (MDS) and its relevance in biology
   + Explain the aims and motivation behind Principal Component Analysis (PCA) and its relevance in biology
   + Write R code to carry out PCA
   + Interpret the effectively communicate the output of PCA

### Suggested reading (optional material expanding on the concepts taught)



[Little book for Multivariate Analysis](https://little-book-of-r-for-multivariate-analysis.readthedocs.io/en/latest/index.html)

[Gene Expression Profiling in Breast Cancer](https://bioconductor.org/packages/release/bioc/vignettes/PCAtools/inst/doc/PCAtools.html#quick-start)

['explor' is an R package to allow interactive exploration of multivariate analysis results](https://juba.github.io/explor/)


## MDS (minimizes dimensions, preserving distance between data points)

Reduction of dimensions is needed when there are far too many features in a dataset, making it hard to distinguish between the important ones that are relevant to the output and the redundant or not-so important ones. Reducing the dimensions of data is called **dimensionality reduction.**

**So the aim is to find the best low-dimensional representation of the variation in a multivariate (lots and lots of variables) data set**, but how do we do this? 


### PCA (minimizes dimensions, preserving covariance of data).


**PCA is a special case of MDS if the distances used in MDS are Euclidean**

<iframe width="560" height="315" src="https://www.youtube.com/embed/GEn-_dAyYME" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


Thinking in 2D is enough for me so let's imagine your data (black dots) is the shape of this baguette. 

![](../gifs/bread_data.png)

If we were to "plot" this baguette (the data) on a "normal" axis the spread would not align in either the x- or y-direction. Our aim is to find a smaller number of variables (fewer than we started with) that capture most of the variation (spread). Where do you think most of the variation lies with the baguette? 

To find these new variables (principal components) we essentially rotate the "normal" axes until we find a suitable set of principal components. Remember the idea is to **reduce** the number of variables so we want to find the set of principal components where the fewest number explain the most amount of variance. Each of these new variables (principal components) is a linear combination of all or some of the original variables. 


![](../gifs/pca.gif)

It looks like we can explain a lot of the variation in the baguette (data) by the first principal component (PC1) alone. How do we "find" these principal components? Simply be minimising the distance between each datapoint and the rotating axes (well okay the computer does this bit). Essentially we've rotated the axes around until we've found the position where the perpendicular distances (blue lines below) between the data (black dots below) and the rotating axes (grey lines, solid (PC1) and dashed (PC2), below) are smallest (black lines, solid (PC1) and dashed (PC2), below). Makes more sense when we think back to the baguette? How would you skewer it to cover most of its length?

![](../gifs/perp.gif)

### Formally...

Principal component analysis is a technique that combines our input variables in a specific way so that we can drop the least important while still retaining the most valuable parts of all of the variables (i.e., those that explain the most variation of the data). PCA results in developing new features that are independent of one another (think of the rotating axes above). The principal components (PC) refer to the new variables constructed as a linear combination of initial variables, such that these new variables are uncorrelated. Since the principal components are independent of one another, they are perpendicular to each other in the Cartesian space.

[The Mathematics Behind Principal Component Analysis (6 min read)](https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643)

## TL;DR
 
 
  + Scale the data by subtracting the mean and dividing by std. deviation.
  + Compute the covariance matrix.
  + Compute <strong>eigenvectors</strong> and the corresponding <strong>eigenvalues</strong>.
  + Sort the <strong>eigenvectors</strong> by decreasing <strong>eigenvalues</strong> and choose k <strong>eigenvectors</strong> with the largest <strong>eigenvalues</strong>, these becoming the <strong>principal components (PCs)</strong>.
  + Derive the new axes by re-orientation of data points according to the <strong>principal components</strong>.

Important points to note:

  + PCA tries to compress as much information as possible in the first PC, the rest in the second, and so onâ€¦
  + <strong>PCs</strong> do not have an interpretable meaning, being a linear combination of features.
  + <strong>Eigenvectors</strong> of the covariance matrix are actually directions of the axes where there is most variance.

Three basic types of information we obtain from Principal Component Analysis:

<strong>PC scores:</strong> the coordinates of our samples on the new PC axis

<strong>Eigenvalues:</strong> represent the variance explained by each PC; we can use these to calculate the proportion of variance in the original data that each axis explains

<strong>Variable loadings (eigenvectors):</strong> these reflect the weight that each variable has on a particular PC (can be thought of as the correlation between the PC and the original variable) 

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">About to explain PCA like...<br>ðŸ˜ŽðŸŒˆðŸ§½ <a href="https://t.co/v56d6qeiK5">pic.twitter.com/v56d6qeiK5</a></p>&mdash; Chelsea Parlett-Pelleriti (@ChelseaParlett) <a href="https://twitter.com/ChelseaParlett/status/1254529361253810177?ref_src=twsrc%5Etfw">April 26, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


## Examples and `R` code 

### Computuing principal components and interpretation

```{r pca, echo = TRUE}
## load data (from base R)
## finger lenghts and body heights of criminals!
data("crimtab") 
?crimtab ## details of the crimtab data
## inspect the data
head(crimtab) 
dim(crimtab) 
str(crimtab) 
colnames(crimtab)
## check the variance accross the variables (columns)
apply(crimtab,2,var)
## **********************************
## carry out principal component analysis on the crimtab data
## to compute the principal components
pca  <- prcomp(crimtab,center = TRUE) 
summary(pca)
## You obtain 22 principal components, which are called PC1-22. 
## Each of these explains a percentage of the total variation in the dataset.
## That is to say: PC1 explains 75% of the total variance, 
## which means that nearly three-quarters of the information in the dataset 
## (22  variables) can be encapsulated by just that one Principal Component. 
## PC2 explains 17% of the variance. So, by knowing the position of a sample 
## in relation to just PC1 and PC2, you can get a very accurate view on where 
## it stands in relation to other samples, as just PC1 and PC2 can explain 
## 92% of the variance.
## use base graphics to show variable importance
## **********************************
## Call str() to have a look at your PCA object
str(pca)
## The PCA object contains the following information:
## The center point ($center), scaling ($scale), standard deviation($sdev) 
## of each principal component.
## The relationship (correlation etc) between the initial variables 
## and the principal components ($rotation)
## The values of each sample in terms of the principal components ($x)
## **********************************
## Now let's make a biplot. This includes both the position of 
## each sample in terms of PC1 and PC2 and also will show you 
## how the initial variables map onto this.
## Using base R graphics
plot(pca)
biplot (pca , scale = 0) 
screeplot(pca)
## Now we will use ggplot2 remember we first need to install it!
## we also need ggfortify to deal with the pca object
library(ggfortify) ## run install.packages("ggfortify") first if not installed
library(ggplot2) ## run install.packages("ggplot2") first if not installed
## Now call autoplot on the pca object
autoplot(pca)
## Passing loadings = TRUE draws eigenvectors.
autoplot(pca,loadings = TRUE)
## You can also attach eigenvector labels and change some options.
autoplot(pca,loadings = TRUE,loadings.label = TRUE, loadings.colour = "grey")
## By default, each component are scaled as the same as standard biplot. 
## You can disable the scaling by specifying scale = 0
autoplot(pca,loadings = TRUE,loadings.label = TRUE, 
         loadings.colour = "grey",scale = 0)
## Now you can see which observations cluster together
## kind of obvious as we're talking about body height :-)
## What about other principal components, let's plot PC3 against PC4
## What do you notice?
autoplot(pca,x = 3, y = 4,loadings = TRUE,loadings.label = TRUE, 
         loadings.colour = "grey",scale = 0)

```

### Multidimensional Scaling in R (for interest)

```{r mds,echo = TRUE}
## Plotting Multidimensional Scaling (for interest)
## stats::cmdscale performs Classical MDS
data("eurodist") ## road distances (in km) between 21 cities in Europe.
?eurodist 
autoplot(eurodist)
## Plotting Classical (Metric) Multidimensional Scaling
autoplot(cmdscale(eurodist, eig = TRUE))
autoplot(cmdscale(eurodist, eig = TRUE), label = TRUE, shape = FALSE,
         label.size = 3)
## Plotting Non-metric Multidimensional Scaling
## MASS::isoMDS and MASS::sammon perform Non-metric MDS
library(MASS)
autoplot(sammon(eurodist))
autoplot(sammon(eurodist), shape = FALSE, label = TRUE,label.size = 3)
## Have a go at interpreting these plots based on the geography of the cities :-)
```

### PCA using RNAseq data

Go through these code chunks and see if you can work out what each chunk is doing. Watch the associated course videos and comment on the output. Try running everything in your own R session and improve the visualisations of the results.

```{r,echo = TRUE}
## https://tavareshugo.github.io/data-carpentry-rnaseq/03_rnaseq_pca.html
## read in rna sequencing data from URL
data <- read.csv(url("https://raw.githubusercontent.com/tavareshugo/data-carpentry-rnaseq/master/data/counts_transformed.csv"))
## let's have a look at the structure
str(data)
## dimensions
dim(data)
## class
class(data)
## Transform the data.frame (excluding the gene column) into a matrix and transpose the matrix so that rows = samples and columns = variables (this is the format prcomp() expects)
data <- t(as.matrix(data[,-1]))
## You could also use tidyverse syntax e.g.,library(plyr) pca_matrix <- trans_cts %>% 
  ## # exclude the gene column
  ## select(-gene) %>% 
  ##  # coerce to a matrix
  ## as.matrix()
## structure?
str(data)
## rownames
rownames(data)
```

Now the data is in the correct shape let's perform the PCA

```{r pca rna,echo = TRUE}
pca <- prcomp(data)
## Summary of PCA
summary(pca)
## scree plot (proportion of variance explained)
## Looks like the first two PCs alone explain the majority of the variation in the data./

screeplot(pca,type = "lines")
```


## Lab work (worth 6\% of your final grade)

You can work in groups or alone; however your work should be independent. Using a dataset of your choice (e.g., that above, or the wine dataset used in the lecture slides...) you should make a cheatsheet illustrating the concepts of PCA and the steps required in the analysis. Please head over to the Assignments page of CANVAS for more details.

You can find some [RStudio cheatsheet examples here](https://rstudio.com/resources/cheatsheets/). You may include `R` code and any plots you feel necessary. The text should be minimal, but easy to follow by your peers. This assignment will be marked by your peers based and should 1) help users find essential information quickly, and 2) be easy to follow! 


 + Be very concise; rely on diagrams where possible.

 + Pay attention to the details! 

 + Code comments inform, but fail to draw the readers attention. It is better to use arrows, speech bubbles, etc. for important information. If it is not important information, leave it out.

 + Simple working examples are more helpful than documentation details. 

 + Add some concise text to help the user make sense of your sections and diagrams and inferences. 
 
You may use whatever software you want to create your cheatsheet. However, [here is a basic PowerPoint template](https://rstudio.com/resources/cheatsheets/how-to-contribute-a-cheatsheet/) you may wish to modify.

You will be expected to peer review 3 other cheatsheets; this also means that your work will be peer marked by 3 of your classmates. The cheatsheets you will review will be automatically assigned to you after the assignment due date. When carrying out this peer review please follow the rubric carefully and be mindful that your comments, although anonymous, will be passed on to your peer. Note that student peer reviews will be considered complete when students have commented at least once on the page and filled out the rubric form for each assignment.